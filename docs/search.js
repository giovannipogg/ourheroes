window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "ourheroes", "modulename": "ourheroes", "type": "module", "doc": "<p>OURHEROES.</p>\n\n<p>This package aims at reproducing the HEROES model by Zhu et al. as described in the paper\nSummarizing Long-Form Document with Rich Discourse Information.</p>\n\n<p>The project is part of the Deep Natural Language Processing course\noffered by Politecnico di Torino, A.Y. 2021-2022.</p>\n\n<p>Authors:\nG. Calleris,\nG. Poggio</p>\n"}, {"fullname": "ourheroes.data", "modulename": "ourheroes.data", "type": "module", "doc": "<p>Module implementing pytorch datasets and data preprocessing.</p>\n\n<p>Classes and functions in this module are used to load and prepare\nsource and target data used for training and evaluating the model.</p>\n"}, {"fullname": "ourheroes.data.datasets", "modulename": "ourheroes.data.datasets", "type": "module", "doc": "<p>Module implementing datasets used for training and evaluating our models.</p>\n\n<p>The datasets in this module implement preprocessing and label generation routines\nused for training and evaluating the model.</p>\n"}, {"fullname": "ourheroes.data.datasets.DocumentDataset", "modulename": "ourheroes.data.datasets", "qualname": "DocumentDataset", "type": "class", "doc": "<p>DocumentDataset(files: List[Union[str, bytes, os.PathLike]], loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]])</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "ourheroes.data.datasets.DocumentDataset.__init__", "modulename": "ourheroes.data.datasets", "qualname": "DocumentDataset.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.datasets.DocumentDataset.loader", "modulename": "ourheroes.data.datasets", "qualname": "DocumentDataset.loader", "type": "variable", "doc": "<p>A Dataset for accessing documents as <code>Document</code> type.    </p>\n\n<p>Attributes:\n    files: The files included in the dataset.\n    loader: The <code>BaseLoader</code> returning a <code>Data</code> object.</p>\n", "annotation": ": Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]]"}, {"fullname": "ourheroes.data.datasets.CRDataset", "modulename": "ourheroes.data.datasets", "qualname": "CRDataset", "type": "class", "doc": "<p>CRDataset(files: List[Union[str, bytes, os.PathLike]], loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]], parser: Callable[[Dict[str, Any]], Tuple[List[Tuple[str, List[str]]], List[str]]], scorer: ourheroes.data.scoring.scorers.CRScorer, tensorizer: ourheroes.data.preprocessing.recursor.Recursor, preprocess: Optional[ourheroes.data.preprocessing.recursor.Recursor] = None)</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "ourheroes.data.datasets.CRDataset.__init__", "modulename": "ourheroes.data.datasets", "qualname": "CRDataset.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]],\n    parser: Callable[[Dict[str, Any]], Tuple[List[Tuple[str, List[str]]], List[str]]],\n    scorer: ourheroes.data.scoring.scorers.CRScorer,\n    tensorizer: ourheroes.data.preprocessing.recursor.Recursor,\n    preprocess: Optional[ourheroes.data.preprocessing.recursor.Recursor] = None\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.datasets.CRDataset.preprocess", "modulename": "ourheroes.data.datasets", "qualname": "CRDataset.preprocess", "type": "variable", "doc": "<p>A Dataset for training the content ranking module.    </p>\n\n<p>Attributes:\n    files: The files included in the dataset.\n    loader: The <code>BaseLoader</code> returning a <code>Data</code> object.\n    parser: The parser for retrieving Document and Summary from Data.\n    preprocess: The shared Document and Summary normalization recursor.\n    scorer: The scorer for generating the content ranking target.\n    tensorizer: The recursor generating the DocumentTensors.</p>\n", "annotation": ": Optional[ourheroes.data.preprocessing.recursor.Recursor]", "default_value": " = None"}, {"fullname": "ourheroes.data.datasets.DigestableDataset", "modulename": "ourheroes.data.datasets", "qualname": "DigestableDataset", "type": "class", "doc": "<p>DigestableDataset(files: List[str], loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]], parser: Callable[[Dict[str, Any]], List[Tuple[str, List[str]]]], tensorizer: ourheroes.data.preprocessing.recursor.Recursor)</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "ourheroes.data.datasets.DigestableDataset.__init__", "modulename": "ourheroes.data.datasets", "qualname": "DigestableDataset.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    files: List[str],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]],\n    parser: Callable[[Dict[str, Any]], List[Tuple[str, List[str]]]],\n    tensorizer: ourheroes.data.preprocessing.recursor.Recursor\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.datasets.DigestableDataset.tensorizer", "modulename": "ourheroes.data.datasets", "qualname": "DigestableDataset.tensorizer", "type": "variable", "doc": "<p>A Dataset for retrieving documents' data to produce digests.    </p>\n\n<p>Attributes:\n    files: The files included in the dataset.\n    loader: The <code>BaseLoader</code> returning a <code>Data</code> object.\n    parser: The parser retrieving the Document\n    preprocess: The parser in charge of normalizing and\n        generating the DocumentTensors.</p>\n", "annotation": ": ourheroes.data.preprocessing.recursor.Recursor"}, {"fullname": "ourheroes.data.datasets.GSDataset", "modulename": "ourheroes.data.datasets", "qualname": "GSDataset", "type": "class", "doc": "<p>GSDataset(files: List[str], loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]], parser: Callable[[Dict[str, Any]], Tuple[List[List[str]], List[str], Dict[int, List[int]]]], scorer: ourheroes.data.scoring.scorers.GSScorer, boundary_encoder: ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder, tokenizer: ourheroes.data.preprocessing.recursor.Recursor, tensorizer: ourheroes.data.preprocessing.recursor.Recursor, grapher: ourheroes.data.graphing.grapher.GSGrapher, eval: bool = False, preprocess: Optional[ourheroes.data.preprocessing.recursor.Recursor] = None)</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "ourheroes.data.datasets.GSDataset.__init__", "modulename": "ourheroes.data.datasets", "qualname": "GSDataset.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    files: List[str],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]],\n    parser: Callable[[Dict[str, Any]], Tuple[List[List[str]], List[str], Dict[int, List[int]]]],\n    scorer: ourheroes.data.scoring.scorers.GSScorer,\n    boundary_encoder: ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder,\n    tokenizer: ourheroes.data.preprocessing.recursor.Recursor,\n    tensorizer: ourheroes.data.preprocessing.recursor.Recursor,\n    grapher: ourheroes.data.graphing.grapher.GSGrapher,\n    eval: bool = False,\n    preprocess: Optional[ourheroes.data.preprocessing.recursor.Recursor] = None\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.datasets.GSDataset.eval", "modulename": "ourheroes.data.datasets", "qualname": "GSDataset.eval", "type": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": " = False"}, {"fullname": "ourheroes.data.datasets.GSDataset.preprocess", "modulename": "ourheroes.data.datasets", "qualname": "GSDataset.preprocess", "type": "variable", "doc": "<p>A Dataset for training the graph summarization module.    </p>\n\n<p>Attributes:\n    files: The files included in the dataset.\n    loader: The <code>BaseLoader</code> returning a <code>Data</code> object.\n    preprocess: The text normalization routine.\n    scorer: The GSScorer generating the target of the\n        graph summarization module.\n    boundary_encoder: The sentence level positional encoder.\n    tokenizer: The tokenizer.\n    tensorizer: The routine for generating tokens' tensors.\n    grapher: The GSGrapher generating the source graph for\n        training the content ranking module.\n    eval: Whether the dataset is used for evaluation (if eval=True),\n        default is training (eval=False).</p>\n", "annotation": ": Optional[ourheroes.data.preprocessing.recursor.Recursor]", "default_value": " = None"}, {"fullname": "ourheroes.data.factories", "modulename": "ourheroes.data.factories", "type": "module", "doc": "<p>Module implementing factory methods for default parsers and datasets.</p>\n\n<p>Functions in this module have default arguments intended to work\nwith documents as formatted in the PubMed and Arxiv datasets,\nand return the default instance of the object they create.</p>\n"}, {"fullname": "ourheroes.data.factories.default_title_parser", "modulename": "ourheroes.data.factories", "qualname": "default_title_parser", "type": "function", "doc": "<p>Creates a <code>DataParser</code> parsing the sections titles.</p>\n\n<p>Args:\n    titles_field: The name of the sections titles field within the expected data.</p>\n\n<p>Returns:\n    A DataParser parsing the sections' titles.</p>\n", "signature": "(\n    titles_field: str = 'section_names'\n) -> ourheroes.data.parsers.DataParser[typing.List[str]]", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_section_parser", "modulename": "ourheroes.data.factories", "qualname": "default_section_parser", "type": "function", "doc": "<p>Creates a <code>DataParser</code> parsing the sections text.</p>\n\n<p>Args:\n    sections_field: The name of the sections field within the expected data.</p>\n\n<p>Returns:\n    A DataParser parsing the sections' literals.</p>\n", "signature": "(\n    sections_field: str = 'sections'\n) -> ourheroes.data.parsers.DataParser[typing.List[typing.List[str]]]", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_summary_parser", "modulename": "ourheroes.data.factories", "qualname": "default_summary_parser", "type": "function", "doc": "<p>Creates a <code>DataParser</code> parsing the summary text.</p>\n\n<p>Args:\n    summary_field: The name of the summary field within the expected data.</p>\n\n<p>Returns:\n    A DataParser parsing the summary literal.</p>\n", "signature": "(\n    summary_field: str = 'abstract_text'\n) -> ourheroes.data.parsers.DataParser[typing.List[str]]", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_document_parser", "modulename": "ourheroes.data.factories", "qualname": "default_document_parser", "type": "function", "doc": "<p>Creates a <code>DocumentParser</code> parsing the document.</p>\n\n<p>Args:\n    titles_field: The name of the sections' titles field within the expected data.\n    sections_field: The name of the sections field within the expected data.</p>\n\n<p>Returns:\n    A DocumentParser parsing the document.</p>\n", "signature": "(\n    titles_field: str = 'section_names',\n    sections_field: str = 'sections'\n) -> ourheroes.data.parsers.DocumentParser", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_cr_parser", "modulename": "ourheroes.data.factories", "qualname": "default_cr_parser", "type": "function", "doc": "<p>Creates a <code>CRParser</code> parsing the document and its summary.</p>\n\n<p>Args:\n    titles_field: The name of the sections' titles field within the expected data.\n    sections_field: The name of the sections field within the expected data.\n    summary_field: The name of the summary field within the expected data.</p>\n\n<p>Returns:\n    A CRParser parsing the document and its summary.</p>\n", "signature": "(\n    titles_field: str = 'section_names',\n    sections_field: str = 'sections',\n    summary_field: str = 'abstract_text'\n) -> ourheroes.data.parsers.CRParser", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_digest_parser", "modulename": "ourheroes.data.factories", "qualname": "default_digest_parser", "type": "function", "doc": "<p>Creates a <code>DigestParser</code> parsing the digest of a document.</p>\n\n<p>Args:\n    digest_field: The name of the digest field within the expected data.</p>\n\n<p>Returns:\n    A DigestParser parsing the document digest.</p>\n", "signature": "(digest_field: str = 'digest') -> ourheroes.data.parsers.DigestParser", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_gs_parser", "modulename": "ourheroes.data.factories", "qualname": "default_gs_parser", "type": "function", "doc": "<p>Creates a <code>GSParser</code> parsing the sections, summary and digest of a document.</p>\n\n<p>Args:\n    sections_field: The name of the sections field within the expected data.\n    summary_field: The name of the summary field within the expected data.\n    digest_field: The name of the digest field within the expected data.</p>\n\n<p>Returns:\n    A GSParser parsing the sections, summary and digest of a document.</p>\n", "signature": "(\n    sections_field: str = 'sections',\n    summary_field: str = 'abstract_text',\n    digest_field: str = 'digest'\n) -> ourheroes.data.parsers.GSParser", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_cr_tensorizer", "modulename": "ourheroes.data.factories", "qualname": "default_cr_tensorizer", "type": "function", "doc": "<p>Creates a <code>Recursor</code> object for mapping sentences to tensors.</p>\n\n<p>Returns:\n    A Recursor object for mapping <code>Sequence</code> to tensor, i.e.\n    conversion of possibly nested sequences of normalized <code>Sentence=str</code> to the\n    same nesting and type sequences of <code>torch.Tensor</code> in the shape\n    <code>(number_of_words, embedding_dimension)</code>.</p>\n", "signature": "() -> ourheroes.data.preprocessing.recursor.Recursor", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_digestable_preprocess", "modulename": "ourheroes.data.factories", "qualname": "default_digestable_preprocess", "type": "function", "doc": "<p>Creates a <code>Recursor</code> object for converting a document to a digestable format.</p>\n\n<p>Returns:\n    A Recursor object for <code>Sequence</code>, i.e.\n    normalization and conversion of possibly nested sequences of <code>Sentence=str</code>\n    to the same nesting and type sequences of <code>torch.Tensor</code> in the shape\n    <code>(number_of_words, embedding_dimension)</code>.</p>\n", "signature": "() -> ourheroes.data.preprocessing.recursor.Recursor", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_document_dataset", "modulename": "ourheroes.data.factories", "qualname": "default_document_dataset", "type": "function", "doc": "<p>Creates a <code>DocumentDataset</code> object given <code>files</code>.</p>\n\n<p>Args:\n    files: The files in the dataset.\n    loader: The BaseLoader implemented by the dataset.</p>\n\n<p>Returns:\n    A DocumentDataset accessing <code>files</code> by means of <code>loader</code>.</p>\n", "signature": "(\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]] = <function load_json>\n) -> ourheroes.data.datasets.DocumentDataset", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_cr_dataset", "modulename": "ourheroes.data.factories", "qualname": "default_cr_dataset", "type": "function", "doc": "<p>Creates a <code>CRDataset</code> object with default attributes given <code>files</code>.</p>\n\n<p>Args:\n    files: The files in the dataset.\n    loader: The BaseLoader implemented by the dataset.</p>\n\n<p>Returns:\n    A CRDataset accessing <code>files</code> by means of <code>loader</code>, and\n    implementing the default parser, preprocessing and scoring.</p>\n", "signature": "(\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]] = <function load_json>\n) -> ourheroes.data.datasets.CRDataset", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_digestable_dataset", "modulename": "ourheroes.data.factories", "qualname": "default_digestable_dataset", "type": "function", "doc": "<p>Creates a <code>DigestableDataset</code> object with default attributes given <code>files</code>.</p>\n\n<p>Args:\n    files: The files in the dataset.\n    loader: The BaseLoader implemented by the dataset.</p>\n\n<p>Returns:\n    A DigestableDataset accessing <code>files</code> by means of <code>loader</code>, and\n    implementing the default parser and preprocessing.</p>\n", "signature": "(\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]] = <function load_json>\n) -> ourheroes.data.datasets.DigestableDataset", "funcdef": "def"}, {"fullname": "ourheroes.data.factories.default_gs_dataset", "modulename": "ourheroes.data.factories", "qualname": "default_gs_dataset", "type": "function", "doc": "<p>Creates a <code>GSDataset</code> object with default attributes given <code>files</code>.</p>\n\n<p>Args:\n    files: The files in the dataset.\n    loader: The BaseLoader implemented by the dataset.\n    val: Whether the dataset is used for evaluation.</p>\n\n<p>Returns:\n    A GSDataset accessing <code>files</code> by means of <code>loader</code>, and\n    implementing the default parser, preprocessing and feature extraction.</p>\n", "signature": "(\n    files: List[Union[str, bytes, os.PathLike]],\n    loader: Callable[[Union[str, bytes, os.PathLike]], Dict[str, Any]] = <function load_json>,\n    val: bool = False\n) -> ourheroes.data.datasets.DigestableDataset", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing", "modulename": "ourheroes.data.graphing", "type": "module", "doc": "<p>Module for document graphs generation.</p>\n\n<p>This module implements the GSGrapher callable class and the utility\n functions associated with it, which are used for generating the\n document representations used in the graph summarization phase\n of the model.</p>\n"}, {"fullname": "ourheroes.data.graphing.grapher", "modulename": "ourheroes.data.graphing.grapher", "type": "module", "doc": "<p>Module implementing the GSGrapher class for document graphs creation.</p>\n"}, {"fullname": "ourheroes.data.graphing.grapher.GSGrapher", "modulename": "ourheroes.data.graphing.grapher", "qualname": "GSGrapher", "type": "class", "doc": "<p>GSGrapher(not_graphed: Union[Set[str], Dict[str, int]])</p>\n"}, {"fullname": "ourheroes.data.graphing.grapher.GSGrapher.__init__", "modulename": "ourheroes.data.graphing.grapher", "qualname": "GSGrapher.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, not_graphed: Union[Set[str], Dict[str, int]])", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.grapher.GSGrapher.not_graphed", "modulename": "ourheroes.data.graphing.grapher", "qualname": "GSGrapher.not_graphed", "type": "variable", "doc": "<p>Utility class for generating the graph summarization source graph.</p>\n\n<p>Attributes:\n    punctuation: The tokens to be considered punctuation.\n    stop_words: The tokens considered stopwords.</p>\n", "annotation": ": Union[Set[str], Dict[str, int]]"}, {"fullname": "ourheroes.data.graphing.grapher.GSGrapher.nodes", "modulename": "ourheroes.data.graphing.grapher", "qualname": "GSGrapher.nodes", "type": "function", "doc": "<p>Generates and adds nodes to G.</p>\n\n<p>Section, sentence and word level nodes are generated as described\nin the reference paper; projection of the sentence and section nodes\nfeatures is performed by the model and punctuation and stop-word tokens\nare not included.</p>\n\n<p>Args:\n    G: The graph representing the document.\n    sections: The tokenized sections of the document.\n    tensors: The embedding of the document's tokens.\n    boundaries: The positional encoding of sentences within sections.</p>\n", "signature": "(\n    self,\n    G: networkx.classes.digraph.DiGraph,\n    sections: List[List[List[Union[int, str]]]],\n    tensors: List[List[List[torch.Tensor]]],\n    boundaries: Dict[Tuple[int, int], torch.Tensor]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.grapher.GSGrapher.graph", "modulename": "ourheroes.data.graphing.grapher", "qualname": "GSGrapher.graph", "type": "function", "doc": "<p>Generates the graph for the summarization module given the document features.</p>\n\n<p>Args:\n    document: The tokenized sections.\n    tensors: The tokens' tensors.\n    boundaries: The positional encoding of sentences within sections.\nReturns:\n    The directed graph representing the document.</p>\n", "signature": "(\n    self,\n    document: List[List[List[Union[int, str]]]],\n    tensors: List[List[List[torch.Tensor]]],\n    boundaries: Dict[Tuple[int, int], torch.Tensor]\n) -> networkx.classes.digraph.DiGraph", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils", "modulename": "ourheroes.data.graphing.utils", "type": "module", "doc": "<p>Module implementing utility functions for document graphs creation.</p>\n"}, {"fullname": "ourheroes.data.graphing.utils.get_typed_nodes", "modulename": "ourheroes.data.graphing.utils", "qualname": "get_typed_nodes", "type": "function", "doc": "<p>Returns the nodes of graph <code>G</code> that are of type <code>nodes_type</code>.</p>\n\n<p>Args:\n    G: The graph from which to retrieve the nodes.\n    nodes_type: The type of nodes to retrieve (<code>type</code> attribute of the nodes' data).\n    data: Whether or not to also return the  selected nodes' data.</p>\n\n<p>Returns:\n    A dictionary (if <code>data=True</code>) with nodes as keys and their respective data as values,\n    or a list (if <code>data=False</code>) of nodes, where the nodes have the requested type.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    nodes_type: str,\n    data: bool = False\n) -> Union[Dict, List]", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.get_word_nodes", "modulename": "ourheroes.data.graphing.utils", "qualname": "get_word_nodes", "type": "function", "doc": "<p>Returns the word nodes of graph <code>G</code>.</p>\n\n<p>Args:\n    G: The graph from which to retrieve the nodes.\n    data: Whether or not to also return the  selected nodes' data.</p>\n\n<p>Returns:\n    A dictionary (if <code>data=True</code>) with nodes as keys and their respective data as values,\n    or a list (if <code>data=False</code>) of nodes, where the nodes are of type 'w'.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    data: bool = False\n) -> Union[Dict, List]", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.get_sentence_nodes", "modulename": "ourheroes.data.graphing.utils", "qualname": "get_sentence_nodes", "type": "function", "doc": "<p>Returns the sentence nodes of graph <code>G</code>.</p>\n\n<p>Args:\n    G: The graph from which to retrieve the nodes.\n    data: Whether or not to also return the  selected nodes' data.</p>\n\n<p>Returns:\n    A dictionary (if <code>data=True</code>) with nodes as keys and their respective data as values,\n    or a list (if <code>data=False</code>) of nodes, where the nodes are of type 's'.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    data: bool = False\n) -> Union[Dict, List]", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.get_section_nodes", "modulename": "ourheroes.data.graphing.utils", "qualname": "get_section_nodes", "type": "function", "doc": "<p>Returns the section nodes of graph <code>G</code>.</p>\n\n<p>Args:\n    G: The graph from which to retrieve the nodes.\n    data: Whether or not to also return the  selected nodes' data.</p>\n\n<p>Returns:\n    A dictionary (if <code>data=True</code>) with nodes as keys and their respective data as values,\n    or a list (if <code>data=False</code>) of nodes, where the nodes are of type 'S'.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    data: bool = False\n) -> Union[Dict, List]", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.w2s", "modulename": "ourheroes.data.graphing.utils", "qualname": "w2s", "type": "function", "doc": "<p>Adds to graph <code>G</code> word-to-sentence edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween a word and all the sentences in which it appears.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.s2w", "modulename": "ourheroes.data.graphing.utils", "qualname": "s2w", "type": "function", "doc": "<p>Adds to graph <code>G</code> sentence-to-word edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween a sentence and all the words which appear in it.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.s2s", "modulename": "ourheroes.data.graphing.utils", "qualname": "s2s", "type": "function", "doc": "<p>Adds to graph <code>G</code> sentence-to-sentence edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween all sentences belonging to the same section.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.S2s", "modulename": "ourheroes.data.graphing.utils", "qualname": "S2s", "type": "function", "doc": "<p>Adds to graph <code>G</code> section-to-sentence edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween all sentences and all sections.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.s2S", "modulename": "ourheroes.data.graphing.utils", "qualname": "s2S", "type": "function", "doc": "<p>Adds to graph <code>G</code> sentence-to-sentence edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween all sentences and the section to which they belong.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.graphing.utils.S2S", "modulename": "ourheroes.data.graphing.utils", "qualname": "S2S", "type": "function", "doc": "<p>Adds to graph <code>G</code> section-to-section edges.</p>\n\n<p>As described in the reference paper, these kind of edge is present\nbetween all sections.</p>\n\n<p>Args:\n    G: The graph for which the edges are to be added.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers", "modulename": "ourheroes.data.parsers", "type": "module", "doc": "<p>Module implementing parsers to convert from the <code>Data</code> to a suitable format.</p>\n\n<p>The classes defined in this module are implemented by the\ndatasets for converting a document loaded in the <code>Data</code> format\nto the objects they need.</p>\n"}, {"fullname": "ourheroes.data.parsers.DataParser", "modulename": "ourheroes.data.parsers", "qualname": "DataParser", "type": "class", "doc": "<p>DataParser(field: str)</p>\n", "bases": "typing.Generic[~Parsed]"}, {"fullname": "ourheroes.data.parsers.DataParser.__init__", "modulename": "ourheroes.data.parsers", "qualname": "DataParser.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, field: str)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.DataParser.field", "modulename": "ourheroes.data.parsers", "qualname": "DataParser.field", "type": "variable", "doc": "<p>Generic utility class to parse a field from data.</p>\n\n<p>The class is intended for generalization purposes:\nalthough in the PubMed and Arxiv datasets documents\nare <code>json</code> files so that retrieval of individual\nfields could be performed directly by key, the\ndataset classes can generalize to other encodings\nby defining new ad hoc parsers.</p>\n\n<p>Attributes:\n    field: The field to be parsed.</p>\n", "annotation": ": str"}, {"fullname": "ourheroes.data.parsers.DataParser.parse", "modulename": "ourheroes.data.parsers", "qualname": "DataParser.parse", "type": "function", "doc": "<p>Returns <code>data[self.field]</code>.</p>\n\n<p>Args:\n    data: The data to be parsed i.e a dictionary with string keys.</p>\n\n<p>Returns:\n    The value <code>data[self.field]</code> of generic type <code>Parsed</code>.</p>\n", "signature": "(self, data: Dict[str, Any]) -> ~Parsed", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.DocumentParser", "modulename": "ourheroes.data.parsers", "qualname": "DocumentParser", "type": "class", "doc": "<p>DocumentParser(title_parser: Callable[[Dict[str, Any]], List[str]], section_parser: Callable[[Dict[str, Any]], List[List[str]]])</p>\n"}, {"fullname": "ourheroes.data.parsers.DocumentParser.__init__", "modulename": "ourheroes.data.parsers", "qualname": "DocumentParser.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    title_parser: Callable[[Dict[str, Any]], List[str]],\n    section_parser: Callable[[Dict[str, Any]], List[List[str]]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.DocumentParser.section_parser", "modulename": "ourheroes.data.parsers", "qualname": "DocumentParser.section_parser", "type": "variable", "doc": "<p>Parses a document from <code>Data</code> to <code>Document</code> type.</p>\n\n<p>The class is callable and implemented by datasets to\nretrieve from the document in the <code>Data</code> format its\n<code>Document</code> type representation, i.e. a <code>List[Tuple[\nTitle, Section]]=List[str, List[str]]</code></p>\n\n<p>Attributes:\n    title_parser: The parser returning the sections' titles.\n    section_parser: The parser returning the sections' text.</p>\n", "annotation": ": Callable[[Dict[str, Any]], List[List[str]]]"}, {"fullname": "ourheroes.data.parsers.DocumentParser.parse", "modulename": "ourheroes.data.parsers", "qualname": "DocumentParser.parse", "type": "function", "doc": "<p>Parses <code>data</code> of type <code>Data</code> into a document of type <code>Document</code>.</p>\n\n<p>Args:\n    data: The data to be parsed i.e a dictionary with string keys.</p>\n\n<p>Returns:\n    The document as type <code>Document=List[Tuple[Title, Section]]=List[str, List[str]]</code>.</p>\n", "signature": "(self, data: Dict[str, Any]) -> List[Tuple[str, List[str]]]", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.CRParser", "modulename": "ourheroes.data.parsers", "qualname": "CRParser", "type": "class", "doc": "<p>CRParser(document_parser: Callable[[Dict[str, Any]], List[Tuple[str, List[str]]]], summary_parser: Callable[[Dict[str, Any]], List[str]])</p>\n"}, {"fullname": "ourheroes.data.parsers.CRParser.__init__", "modulename": "ourheroes.data.parsers", "qualname": "CRParser.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    document_parser: Callable[[Dict[str, Any]], List[Tuple[str, List[str]]]],\n    summary_parser: Callable[[Dict[str, Any]], List[str]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.CRParser.summary_parser", "modulename": "ourheroes.data.parsers", "qualname": "CRParser.summary_parser", "type": "variable", "doc": "<p>Parses a document from <code>Data</code> to a <code>Tuple[Document, Summary]</code>.</p>\n\n<p>The class is implemented by the <code>CRDataset</code>, which is used for\ntraining the content ranking module, thus extracting from a \n<code>Data</code> object the <code>Document</code> and <code>Summary</code> parts.</p>\n\n<p>Attributes:\n    document_parser: The parser returning the document text.\n    summary_parser: The parser returning the summary text.</p>\n", "annotation": ": Callable[[Dict[str, Any]], List[str]]"}, {"fullname": "ourheroes.data.parsers.CRParser.parse", "modulename": "ourheroes.data.parsers", "qualname": "CRParser.parse", "type": "function", "doc": "<p>Parses <code>data</code> of type <code>Data</code> into a <code>Tuple[Document, Summary]</code>.</p>\n\n<p>Args:\n    data: The data to be parsed i.e a dictionary with string keys.</p>\n\n<p>Returns:\n    The <code>Tuple[Document, Summary]</code> where\n    <code>Document=List[Tuple[Title, Section]]=List[str, List[str]]</code>\n    and <code>Summary=List[str]</code>.</p>\n", "signature": "(\n    self,\n    data: Dict[str, Any]\n) -> Tuple[List[Tuple[str, List[str]]], List[str]]", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.DigestParser", "modulename": "ourheroes.data.parsers", "qualname": "DigestParser", "type": "class", "doc": "<p>DigestParser(digest: str)</p>\n"}, {"fullname": "ourheroes.data.parsers.DigestParser.__init__", "modulename": "ourheroes.data.parsers", "qualname": "DigestParser.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, digest: str)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.DigestParser.digest", "modulename": "ourheroes.data.parsers", "qualname": "DigestParser.digest", "type": "variable", "doc": "<p>Parses the digest from data.</p>\n\n<p>The class performs the conversion from <code>str</code> to <code>int</code>\nnecessitated by the original <code>json</code> formatting of the document.</p>\n\n<p>Attributes:\n    digest: The key of the digest within the data.</p>\n", "annotation": ": str"}, {"fullname": "ourheroes.data.parsers.DigestParser.parse", "modulename": "ourheroes.data.parsers", "qualname": "DigestParser.parse", "type": "function", "doc": "<p>Retrieves the digest from <code>data</code>.</p>\n\n<p>Args:\n    data: The data to be parsed i.e a dictionary with string keys.</p>\n\n<p>Returns:\n    The digest in the <code>Digest</code> format i.e. <code>Dict[int, List[int]]</code>.</p>\n", "signature": "(self, data: Dict[str, Any]) -> Dict[int, List[int]]", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.GSParser", "modulename": "ourheroes.data.parsers", "qualname": "GSParser", "type": "class", "doc": "<p>GSParser(section_parser: Callable[[Dict[str, Any]], List[List[str]]], summary_parser: Callable[[Dict[str, Any]], List[str]], digest_parser: Callable[[Dict[str, Any]], Dict[int, List[int]]])</p>\n"}, {"fullname": "ourheroes.data.parsers.GSParser.__init__", "modulename": "ourheroes.data.parsers", "qualname": "GSParser.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    section_parser: Callable[[Dict[str, Any]], List[List[str]]],\n    summary_parser: Callable[[Dict[str, Any]], List[str]],\n    digest_parser: Callable[[Dict[str, Any]], Dict[int, List[int]]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.parsers.GSParser.digest_parser", "modulename": "ourheroes.data.parsers", "qualname": "GSParser.digest_parser", "type": "variable", "doc": "<p>Parses the sections, summary and digest from data.</p>\n\n<p>The class is implemented by the GSDataset class and\nretrieves the necessary for generating the source and\ntarget variables for training the graph summarization\nmodule.</p>\n\n<p>Attributes:\n    section_parser: The parser retrieving sections.\n    summary_parser: The parser retrieving the summary.\n    digest_parser: The parser retrieving the digest.</p>\n", "annotation": ": Callable[[Dict[str, Any]], Dict[int, List[int]]]"}, {"fullname": "ourheroes.data.parsers.GSParser.parse", "modulename": "ourheroes.data.parsers", "qualname": "GSParser.parse", "type": "function", "doc": "<p>Retrieves sections, summary and digest from data.</p>\n\n<p>Args:\n    data: The data to be parsed i.e a dictionary with string keys.</p>\n\n<p>Returns:\n    The <code>Tuple[List[Section], Summary, Digest]=\n    Tuple[List[List[str]], List[str], Dict[int, List[int]]]</code>\n    retrieved from <code>data</code>.</p>\n", "signature": "(\n    self,\n    data: Dict[str, Any]\n) -> Tuple[List[List[str]], List[str], Dict[int, List[int]]]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing", "modulename": "ourheroes.data.preprocessing", "type": "module", "doc": "<p>Module implementing text reprocessing utility classes and functions.</p>\n\n<p>The content of this module is implemented in the pytorch datasets used\nat the time of training.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "type": "module", "doc": "<p>Module implementing the BoundaryEncoder class.</p>\n\n<p>The callable BoundaryEncoder object generates positional encoding\nfor sentences within sections given a document.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "qualname": "BoundaryEncoder", "type": "class", "doc": "<p>BoundaryEncoder(dimension: int = 64)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder.__init__", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "qualname": "BoundaryEncoder.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, dimension: int = 64)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder.dimension", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "qualname": "BoundaryEncoder.dimension", "type": "variable", "doc": "<p>Class for encoding the boundary distances of sentences\nwithin a section.</p>\n\n<p>The encoding is under all aspects analogous to positional\nencoding of tokens within sentences, although in this\ncase it is encoding the position of sentences within sections.</p>\n\n<p>Attributes:\n    dimension: The dimension of the encoding.\n        Since the distances are computed both from the\n        start and from the end of a section, the output\n        dimension will be twice this value.</p>\n", "annotation": ": int", "default_value": " = 64"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder.boundary", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "qualname": "BoundaryEncoder.boundary", "type": "function", "doc": "<p>Computes the positional encoding of a sentence within a section.</p>\n\n<p>Args:\n    sentence: The sentence position in the section.\n    length: The total section length.\nReturns:\n    The positional encoding of the sentence within the section.</p>\n", "signature": "(self, sentence: int, length: int) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.boundary_encoder.BoundaryEncoder.boundaries", "modulename": "ourheroes.data.preprocessing.boundary_encoder", "qualname": "BoundaryEncoder.boundaries", "type": "function", "doc": "<p>Computes the positional encoding of sentences included in the digest.</p>\n\n<p>Args:\n    sections: The literal of the document sections.\n    digest: The digest produce by the trained content ranking module.\nReturns:\n    The positional encodings of the sentences included in the digest\n    within their respective sections.</p>\n", "signature": "(\n    self,\n    sections: List[List[str]],\n    digest: Dict[int, List[int]]\n) -> Dict[Tuple[int, int], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.factories", "modulename": "ourheroes.data.preprocessing.factories", "type": "module", "doc": "<p>Module implementing factory functions used by the preprocessing modules.</p>\n\n<p>The functions in this module return a default instance of the object they create.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.factories.default_tokenizer", "modulename": "ourheroes.data.preprocessing.factories", "qualname": "default_tokenizer", "type": "function", "doc": "<p>Creates the default Tokenizer.</p>\n\n<p>Returns:\n    The nltk word_tokenize function.</p>\n", "signature": "(\n    vocab: Union[Set[str], Dict[str, int], NoneType] = None\n) -> Callable[[str], List[Union[int, str]]]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.factories.default_token_tensorizer", "modulename": "ourheroes.data.preprocessing.factories", "qualname": "default_token_tensorizer", "type": "function", "doc": "<p>Creates the default TokenTensorizer.</p>\n\n<p>Returns:\n    The TokenTensorizerObject implementing\n    torchtext.vocab.GloVe.</p>\n", "signature": "() -> ourheroes.data.preprocessing.tensorizers.TokenTensorizer", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.factories.default_sentence_tensorizer", "modulename": "ourheroes.data.preprocessing.factories", "qualname": "default_sentence_tensorizer", "type": "function", "doc": "<p>Creates the default SentenceTensorizer.\nArgs:\n    start_of_sentence: the start-of-sentence token or None to disable.\n    end_of_sentence: the end-of-sentence token or None to disable.</p>\n\n<p>Returns:\n    The SentenceTensorizer object implementing\n    the default TokenTensorizer.</p>\n", "signature": "(\n    start_of_sentence: Union[int, str] = '<s>',\n    end_of_sentence: Union[int, str] = '</s>'\n) -> ourheroes.data.preprocessing.tensorizers.SentenceTensorizer", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.pipeline", "modulename": "ourheroes.data.preprocessing.pipeline", "type": "module", "doc": "<p>Module implementing the PreprocessingPipeline class.</p>\n\n<p>The PreprocessingPipeline pipeline is our custom class for\nstring preprocessing.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.pipeline.PreprocessingPipeline", "modulename": "ourheroes.data.preprocessing.pipeline", "qualname": "PreprocessingPipeline", "type": "class", "doc": "<p>PreprocessingPipeline(stages: Sequence)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.pipeline.PreprocessingPipeline.__init__", "modulename": "ourheroes.data.preprocessing.pipeline", "qualname": "PreprocessingPipeline.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, stages: Sequence)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.pipeline.PreprocessingPipeline.stages", "modulename": "ourheroes.data.preprocessing.pipeline", "qualname": "PreprocessingPipeline.stages", "type": "variable", "doc": "<p>A preprocessing pipeline.</p>\n\n<p>Attributes:\n    stages: The ordered sequence of stages to apply.</p>\n", "annotation": ": Sequence"}, {"fullname": "ourheroes.data.preprocessing.pipeline.PreprocessingPipeline.preprocess", "modulename": "ourheroes.data.preprocessing.pipeline", "qualname": "PreprocessingPipeline.preprocess", "type": "function", "doc": "<p>Orderly applies all stages to sentence.</p>\n\n<p>Args:\n    sentence: the string to process.\nReturns:\n    The processed sentence.</p>\n", "signature": "(self, sentence: str) -> Union[str, List[Union[int, str]], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.recursor", "modulename": "ourheroes.data.preprocessing.recursor", "type": "module", "doc": "<p>Module implementing the Recursor class.</p>\n\n<p>The Recursor class applies the preprocessing routine given\nat initialization and applies it recursively preserving the\nstructure of the Sequence to preprocess.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.recursor.Recursor", "modulename": "ourheroes.data.preprocessing.recursor", "qualname": "Recursor", "type": "class", "doc": "<p>Recursor(process: Union[Callable[[str], str], Callable[[str], List[Union[int, str]]], Callable[[Union[int, str]], torch.Tensor], Callable[[List[Union[int, str]]], torch.Tensor], ourheroes.data.preprocessing.pipeline.PreprocessingPipeline])</p>\n"}, {"fullname": "ourheroes.data.preprocessing.recursor.Recursor.__init__", "modulename": "ourheroes.data.preprocessing.recursor", "qualname": "Recursor.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    process: Union[Callable[[str], str], Callable[[str], List[Union[int, str]]], Callable[[Union[int, str]], torch.Tensor], Callable[[List[Union[int, str]]], torch.Tensor], ourheroes.data.preprocessing.pipeline.PreprocessingPipeline]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.recursor.Recursor.process", "modulename": "ourheroes.data.preprocessing.recursor", "qualname": "Recursor.process", "type": "variable", "doc": "<p>A utility class for preprocessing nested sequences.</p>\n\n<p>This class is useful for preprocessing nested sequences\nas it applies the process it is initialized with to the\n(inner most) string levels while reconstructing the\nstructure retaining the types.</p>\n\n<p>Attributes:\n    process: The routine applied to strings.</p>\n", "annotation": ": Union[Callable[[str], str], Callable[[str], List[Union[int, str]]], Callable[[Union[int, str]], torch.Tensor], Callable[[List[Union[int, str]]], torch.Tensor], ourheroes.data.preprocessing.pipeline.PreprocessingPipeline]"}, {"fullname": "ourheroes.data.preprocessing.recursor.Recursor.recurse", "modulename": "ourheroes.data.preprocessing.recursor", "qualname": "Recursor.recurse", "type": "function", "doc": "<p>Returns the sequence with processed string.</p>\n\n<p>Args:\n    sequence: The sequence or string to process.\nReturns:\n    The same type(s) and nesting structure with\n        process strings.</p>\n", "signature": "(\n    self,\n    sequence: Union[Sequence, str]\n) -> Union[Sequence, str, List[Union[int, str]], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.remover", "modulename": "ourheroes.data.preprocessing.remover", "type": "module", "doc": "<p>Module implementing the Remover class.</p>\n\n<p>The Remover object removes unwanted n-grams (passed at time of\ninitialization) from a string.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.remover.Remover", "modulename": "ourheroes.data.preprocessing.remover", "qualname": "Remover", "type": "class", "doc": "<p>Remover(removes: Sequence[str])</p>\n"}, {"fullname": "ourheroes.data.preprocessing.remover.Remover.__init__", "modulename": "ourheroes.data.preprocessing.remover", "qualname": "Remover.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, removes: Sequence[str])", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.remover.Remover.removes", "modulename": "ourheroes.data.preprocessing.remover", "qualname": "Remover.removes", "type": "variable", "doc": "<p>A callable remover.</p>\n\n<p>Arguments:\n    removes: the n-grams to be removed when called.</p>\n", "annotation": ": Sequence[str]"}, {"fullname": "ourheroes.data.preprocessing.remover.Remover.remove", "modulename": "ourheroes.data.preprocessing.remover", "qualname": "Remover.remove", "type": "function", "doc": "<p>Removes unwanted n-grams from sentence.</p>\n\n<p>Args:\n    sentence: The sentence to remove from.\nReturns:\n    The cleaned version of sentence.</p>\n", "signature": "(self, sentence: str)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tensorizers", "modulename": "ourheroes.data.preprocessing.tensorizers", "type": "module", "doc": "<p>Module implementing the tensorizer classes.</p>\n\n<p>This classes are used to map tokens and sequence of tokens\nto their GloVe vector embeddings.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.TokenTensorizer", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "TokenTensorizer", "type": "class", "doc": "<p>TokenTensorizer(vectors: torchtext.vocab.vectors.GloVe)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.TokenTensorizer.__init__", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "TokenTensorizer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, vectors: torchtext.vocab.vectors.GloVe)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.TokenTensorizer.vectors", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "TokenTensorizer.vectors", "type": "variable", "doc": "<p>Callable wrapper class for token embedding.</p>\n\n<p>Attributes:\n    vectors: The vectors from which the token embeddings\n        are retrieved (although torchtext.vocab.GloVe is\n        expected, any Mapping[Token, torch.Tensor] satisfies\n        the implementation).</p>\n", "annotation": ": torchtext.vocab.vectors.GloVe"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.TokenTensorizer.tensorize", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "TokenTensorizer.tensorize", "type": "function", "doc": "<p>Returns the embedding of token.</p>\n\n<p>Args:\n    token: The token to embed.\nReturns:\n    The embedding of the token.</p>\n", "signature": "(self, token: Union[int, str]) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.SentenceTensorizer", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "SentenceTensorizer", "type": "class", "doc": "<p>SentenceTensorizer(tensorizer: ourheroes.data.preprocessing.tensorizers.TokenTensorizer, start_of_sentence: Optional[str] = None, end_of_sentence: Optional[str] = None)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.SentenceTensorizer.__init__", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "SentenceTensorizer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    tensorizer: ourheroes.data.preprocessing.tensorizers.TokenTensorizer,\n    start_of_sentence: Optional[str] = None,\n    end_of_sentence: Optional[str] = None\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.SentenceTensorizer.start_of_sentence", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "SentenceTensorizer.start_of_sentence", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]", "default_value": " = None"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.SentenceTensorizer.end_of_sentence", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "SentenceTensorizer.end_of_sentence", "type": "variable", "doc": "<p>Class for mapping a sentence from <code>Tokenized</code> to Tensor.</p>\n\n<p>Attributes:\n    tensorizer: The implemented TokenTensorizer.\n    start_of_sentence: the start-of-sentence token or None to disable.\n    end_of_sentence: the end-of-sentence token or None to disable.</p>\n", "annotation": ": Optional[str]", "default_value": " = None"}, {"fullname": "ourheroes.data.preprocessing.tensorizers.SentenceTensorizer.tensorize", "modulename": "ourheroes.data.preprocessing.tensorizers", "qualname": "SentenceTensorizer.tensorize", "type": "function", "doc": "<p>Returns the embedding of the tokenized sentence.</p>\n\n<p>Args:\n    tokenized: The tokenized sentence to embed.\nReturns:\n    The sequence of tokens as a pytorch Tensor\n        with shape\n        (number_of_tokens, embedding_dimension).</p>\n", "signature": "(self, tokenized: List[Union[int, str]]) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tokenizers", "modulename": "ourheroes.data.preprocessing.tokenizers", "type": "module", "doc": "<p>Module implementing the tokenizer classes.</p>\n\n<p>These classes are used to break up sentences into token vectors.</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.SimpleTokenizer", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "SimpleTokenizer", "type": "class", "doc": "<p>SimpleTokenizer(max_tokens: Optional[int] = None)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.SimpleTokenizer.__init__", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "SimpleTokenizer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, max_tokens: Optional[int] = None)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.SimpleTokenizer.max_tokens", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "SimpleTokenizer.max_tokens", "type": "variable", "doc": "<p>A minimal tokenizer.</p>\n\n<p>Since the Arxiv and PubMed datasets already come\nwith tokenized sentences, this class further\nseparates composite words in order to gain in\nrepresentability.</p>\n\n<p>Attributes:\n    max_tokens: The maximum number of tokens\n        retained when the object is called,\n        after which a sentence is truncated.</p>\n\n<p>Example:\n    Words like 'l-arginine' may not find\n    a representation within GloVe's learned\n    embeddings while the sub-words 'l', '-'\n    and 'arginine' do.</p>\n", "annotation": ": Optional[int]", "default_value": " = None"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.SimpleTokenizer.tokenize", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "SimpleTokenizer.tokenize", "type": "function", "doc": "<p>Splits the sentence into tokens.</p>\n\n<p>Args:\n    sentence: The sentence to tokenize.\nReturns:\n    The tokenized sentence up to max_tokens.</p>\n", "signature": "(self, sentence: str) -> List[Union[int, str]]", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.NLTKTokenizer", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "NLTKTokenizer", "type": "class", "doc": "<p>NLTKTokenizer(tokenizer: Callable[[str], List[Union[int, str]]], vocab: Union[Set[str], Dict[str, int], NoneType] = None, max_tokens: Optional[int] = None)</p>\n"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.NLTKTokenizer.__init__", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "NLTKTokenizer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    tokenizer: Callable[[str], List[Union[int, str]]],\n    vocab: Union[Set[str], Dict[str, int], NoneType] = None,\n    max_tokens: Optional[int] = None\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.NLTKTokenizer.vocab", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "NLTKTokenizer.vocab", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[Set[str], Dict[str, int], NoneType]", "default_value": " = None"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.NLTKTokenizer.max_tokens", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "NLTKTokenizer.max_tokens", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": " = None"}, {"fullname": "ourheroes.data.preprocessing.tokenizers.NLTKTokenizer.tokenize", "modulename": "ourheroes.data.preprocessing.tokenizers", "qualname": "NLTKTokenizer.tokenize", "type": "function", "doc": "<p></p>\n", "signature": "(self, sentence: str) -> List[Union[int, str]]", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring", "modulename": "ourheroes.data.scoring", "type": "module", "doc": "<p>Module implementing the label generation routines.</p>\n\n<p>The classes in this module are implemented by the pytorch datasets used\nfor training in order to generate the target of the models.</p>\n"}, {"fullname": "ourheroes.data.scoring.factories", "modulename": "ourheroes.data.scoring.factories", "type": "module", "doc": "<p>Module implementing factory methods of scorers.</p>\n\n<p>The functions in this module return the default instance of\nthe scorer objects used for target label generation.</p>\n"}, {"fullname": "ourheroes.data.scoring.factories.get_mean_rouge_scorer", "modulename": "ourheroes.data.scoring.factories", "qualname": "get_mean_rouge_scorer", "type": "function", "doc": "<p>Factory method for generating a MeanRougeScorer.</p>\n\n<p>Args:\n    rouge_types: The types of rouge to be evaluated.\n    metric: The metric of which the average is taken by the MeanRougeScorer.\n    use_stemmer: Whether or not the implemented rouge scorer should use a stemmer.\nReturns:\n    The corresponding MeanRougeScorer.</p>\n", "signature": "(\n    rouge_types: Union[str, List[str]],\n    metric: str,\n    use_stemmer: bool = False\n) -> ourheroes.data.scoring.scorers.MeanRougeScorer", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.factories.default_sentence_scorer", "modulename": "ourheroes.data.scoring.factories", "qualname": "default_sentence_scorer", "type": "function", "doc": "<p>Factory method returning the default MeanRougeScorer for evaluating sentences.</p>\n", "signature": "() -> ourheroes.data.scoring.scorers.MeanRougeScorer", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.factories.default_section_scorer", "modulename": "ourheroes.data.scoring.factories", "qualname": "default_section_scorer", "type": "function", "doc": "<p>Factory method returning the default MeanRougeScorer for evaluating sections.</p>\n", "signature": "() -> ourheroes.data.scoring.scorers.MeanRougeScorer", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.factories.default_cr_scorer", "modulename": "ourheroes.data.scoring.factories", "qualname": "default_cr_scorer", "type": "function", "doc": "<p>Factory method returning the default CRScorer.</p>\n", "signature": "() -> ourheroes.data.scoring.scorers.CRScorer", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.factories.default_gs_scorer", "modulename": "ourheroes.data.scoring.factories", "qualname": "default_gs_scorer", "type": "function", "doc": "<p>Factory method returning the default GSScorer.</p>\n", "signature": "() -> ourheroes.data.scoring.scorers.GSScorer", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers", "modulename": "ourheroes.data.scoring.scorers", "type": "module", "doc": "<p>Callable objects generating rouge evaluations and training targets.</p>\n\n<p>The classes are designed to be callable for generalization purposes,\nand are mainly implemented by datasets for generating the training targets.</p>\n"}, {"fullname": "ourheroes.data.scoring.scorers.MeanRougeScorer", "modulename": "ourheroes.data.scoring.scorers", "qualname": "MeanRougeScorer", "type": "class", "doc": "<p>MeanRougeScorer(rouge_types: List[str], metric: str, scorer: rouge_score.rouge_scorer.RougeScorer)</p>\n"}, {"fullname": "ourheroes.data.scoring.scorers.MeanRougeScorer.__init__", "modulename": "ourheroes.data.scoring.scorers", "qualname": "MeanRougeScorer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    rouge_types: List[str],\n    metric: str,\n    scorer: rouge_score.rouge_scorer.RougeScorer\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers.MeanRougeScorer.scorer", "modulename": "ourheroes.data.scoring.scorers", "qualname": "MeanRougeScorer.scorer", "type": "variable", "doc": "<p>A utility class used for rouge evaluations.</p>\n\n<p>The class serves the purpose of generating targets used while training,\nwhich, as described in the reference paper, the mean of a given metric\nof different rouge flavors.-</p>\n\n<p>Attributes:\n    rouge_types: The rouge types from which the average is taken.\n    metric: The metric on which the average is taken.\n    scorer: The rouge evaluator.</p>\n", "annotation": ": rouge_score.rouge_scorer.RougeScorer"}, {"fullname": "ourheroes.data.scoring.scorers.MeanRougeScorer.score", "modulename": "ourheroes.data.scoring.scorers", "qualname": "MeanRougeScorer.score", "type": "function", "doc": "<p>Evaluates the average rouge similarity between target and prediction.</p>\n\n<p>Args:\n    target: The reference phrase or summary.\n    prediction: The evaluated phrase or summary.\nReturns:\n    The mean of the metric of all the rouge flavors as\n    passed at the moment of initialization.</p>\n", "signature": "(self, target: str, prediction: str) -> float", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers.CRScorer", "modulename": "ourheroes.data.scoring.scorers", "qualname": "CRScorer", "type": "class", "doc": "<p>CRScorer(sentence_scorer: Callable[[str, str], float], section_scorer: Callable[[str, str], float])</p>\n"}, {"fullname": "ourheroes.data.scoring.scorers.CRScorer.__init__", "modulename": "ourheroes.data.scoring.scorers", "qualname": "CRScorer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    sentence_scorer: Callable[[str, str], float],\n    section_scorer: Callable[[str, str], float]\n)", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers.CRScorer.section_scorer", "modulename": "ourheroes.data.scoring.scorers", "qualname": "CRScorer.section_scorer", "type": "variable", "doc": "<p>The class generating labels for training the content ranking module.</p>\n\n<p>Attributes:\n    sentence_scorer: The sentence level evaluator.\n    section_scorer: The section level evaluator.</p>\n", "annotation": ": Callable[[str, str], float]"}, {"fullname": "ourheroes.data.scoring.scorers.CRScorer.score", "modulename": "ourheroes.data.scoring.scorers", "qualname": "CRScorer.score", "type": "function", "doc": "<p>Generates labels for the content ranking module given the document and the golden summary.</p>\n\n<p>Args:\n    summary: The document's golden summary.\n    document: The original document.\nReturns:\n    The target labels for training the content ranking module.</p>\n", "signature": "(\n    self,\n    summary: List[str],\n    document: List[Tuple[str, List[str]]]\n) -> Tuple[torch.Tensor, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers.GSScorer", "modulename": "ourheroes.data.scoring.scorers", "qualname": "GSScorer", "type": "class", "doc": "<p>GSScorer(scorer: Callable[[str, str], float], select_n: int)</p>\n"}, {"fullname": "ourheroes.data.scoring.scorers.GSScorer.__init__", "modulename": "ourheroes.data.scoring.scorers", "qualname": "GSScorer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, scorer: Callable[[str, str], float], select_n: int)", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.scorers.GSScorer.select_n", "modulename": "ourheroes.data.scoring.scorers", "qualname": "GSScorer.select_n", "type": "variable", "doc": "<p>The class generating labels for training the graph summarization module.</p>\n\n<p>The targets are binary labels which evaluate to 1 the top n document\nsentences most similar the each sentence in the golden summary. </p>\n\n<p>Attributes:\n    scorer: The base scorer performing the sentence level evaluations.\n    select_n: The number of document sentences to label as positive (1)\n        for each sentence in the summary.</p>\n", "annotation": ": int"}, {"fullname": "ourheroes.data.scoring.scorers.GSScorer.score", "modulename": "ourheroes.data.scoring.scorers", "qualname": "GSScorer.score", "type": "function", "doc": "<p>Generates labels for the graph summarization module given the document's\nsection and the golden summary.</p>\n\n<p>Args:\n    summary: The document's golden summary.\n    sections: The original document sections.\nReturns:\n    The target labels for training the graph summarization module.</p>\n", "signature": "(self, summary: List[str], sections: List[List[str]]) -> List[List[float]]", "funcdef": "def"}, {"fullname": "ourheroes.data.scoring.utils", "modulename": "ourheroes.data.scoring.utils", "type": "module", "doc": "<p>Module implementing utility functions for the scores module.</p>\n"}, {"fullname": "ourheroes.data.scoring.utils.resection", "modulename": "ourheroes.data.scoring.utils", "qualname": "resection", "type": "function", "doc": "<p>Rearranges the scores to reflect the original document structure.</p>\n\n<p>Args:\n    scores: The sentences' scores.\n    sections: The document as originally structured.\nReturns:\n    The scores rearranged as to reflect the original document structure.</p>\n", "signature": "(scores: numpy.ndarray, sections: List[List[str]]) -> List[List[float]]", "funcdef": "def"}, {"fullname": "ourheroes.data.types", "modulename": "ourheroes.data.types", "type": "module", "doc": "<p>Module implementing type aliases for enhanced readability.</p>\n\n<p>The type aliases in this module are used for typing purposes\nas well as for a gain in readability of the code.</p>\n"}, {"fullname": "ourheroes.data.utils", "modulename": "ourheroes.data.utils", "type": "module", "doc": "<p>Module implementing utility functions used by Parsers and Datasets.</p>\n"}, {"fullname": "ourheroes.data.utils.load_json", "modulename": "ourheroes.data.utils", "qualname": "load_json", "type": "function", "doc": "<p>Loads a <code>json</code> file and returns a <code>Data</code> object.</p>\n\n<p>Args:\n    filepath: The location of the file.\n    encoding: The encoding of the file.</p>\n\n<p>Returns:\n    The content of the file as a dictionary with <code>str</code> keys.</p>\n", "signature": "(\n    filepath: Union[str, bytes, os.PathLike],\n    encoding: str = 'utf-8'\n) -> Dict[str, Any]", "funcdef": "def"}, {"fullname": "ourheroes.data.utils.select", "modulename": "ourheroes.data.utils", "qualname": "select", "type": "function", "doc": "<p>Filters the literal document to obtain the literal digest.</p>\n\n<p>Args:\n    document: The document literal, a <code>List[Section]=List[List[str]]</code>.\n    digest: The digest in terms of selected sections' and sentences' numerals.</p>\n\n<p>Returns:\n    The digest literal, of the same type as document.</p>\n", "signature": "(\n    document: List[List[str]],\n    digest: Dict[int, List[int]]\n) -> List[List[str]]", "funcdef": "def"}, {"fullname": "ourheroes.inspection", "modulename": "ourheroes.inspection", "type": "module", "doc": "<p>Module implementing utilities for dataset inspection.</p>\n\n<p>Before conducting our experiments we performed a dataset inspection\nin order to exclude abnormal documents, e.g. those without summaries.\nThis module is used for generating the documents statistics which\nwere analyzed in the Inspection.ipynb notebook present in the\nproject GitHub page.</p>\n"}, {"fullname": "ourheroes.inspection.inspect_files", "modulename": "ourheroes.inspection.inspect_files", "type": "module", "doc": "<p>Module implementing the document inspection routines.</p>\n"}, {"fullname": "ourheroes.inspection.inspect_files.vocab_representability", "modulename": "ourheroes.inspection.inspect_files", "qualname": "vocab_representability", "type": "function", "doc": "<p>Prints information about the dataset vocabulary <code>vocab</code> and the GloVe-representable vocabulary <code>glove</code>.</p>\n", "signature": "(vocab: Dict[str, int], glove: Dict[str, int])", "funcdef": "def"}, {"fullname": "ourheroes.inspection.inspect_files.inspect_file", "modulename": "ourheroes.inspection.inspect_files", "qualname": "inspect_file", "type": "function", "doc": "<p>Generates statistics about <code>file</code>.</p>\n", "signature": "(\n    file: str,\n    representable: Set[str],\n    not_graphed: Set[str],\n    tq: tqdm.std.tqdm,\n    encoding: str = 'utf-8',\n    sections: str = 'sections',\n    summary: str = 'abstract_text',\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n) -> Tuple[str, Dict[str, Any]]", "funcdef": "async def"}, {"fullname": "ourheroes.inspection.inspect_files.inspect_sentence", "modulename": "ourheroes.inspection.inspect_files", "qualname": "inspect_sentence", "type": "function", "doc": "<p>Generates statistics about <code>sentence</code>.</p>\n", "signature": "(\n    sentence: str,\n    representable: Set[str],\n    not_graphed: Set[str],\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n) -> Tuple[int, int, int]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.inspect_files.inspect_sections", "modulename": "ourheroes.inspection.inspect_files", "qualname": "inspect_sections", "type": "function", "doc": "<p>Generates statistics about <code>sections</code>.</p>\n", "signature": "(\n    sections: List[List[str]],\n    representable: Set[str],\n    not_graphed: Set[str],\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n) -> List[List[Tuple[int, int, int]]]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.inspect_files.inspect_summary", "modulename": "ourheroes.inspection.inspect_files", "qualname": "inspect_summary", "type": "function", "doc": "<p>Generates statistics about <code>summary</code>.</p>\n", "signature": "(\n    summary: List[str],\n    representable: Set[str],\n    not_graphed: Set[str],\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n) -> List[Tuple[int, int, int]]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.inspect_files.inspect_files", "modulename": "ourheroes.inspection.inspect_files", "qualname": "inspect_files", "type": "function", "doc": "<p>Generates statistics about dataset <code>files</code>.</p>\n", "signature": "(\n    files: List[str],\n    representable: Set[str],\n    not_graphed: Set[str],\n    save_path: str = 'files\\\\documents.stats',\n    encoding: str = 'utf-8',\n    sections: str = 'sections',\n    summary: str = 'abstract_text',\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n)", "funcdef": "async def"}, {"fullname": "ourheroes.inspection.inspect_files.main", "modulename": "ourheroes.inspection.inspect_files", "qualname": "main", "type": "function", "doc": "<p>Performs the datasets inspection.</p>\n", "signature": "(root: str = 'pubmed-dataset')", "funcdef": "async def"}, {"fullname": "ourheroes.inspection.utils", "modulename": "ourheroes.inspection.utils", "type": "module", "doc": "<p>Module implementing the document inspection utility functions.</p>\n"}, {"fullname": "ourheroes.inspection.utils.get_not_graphed", "modulename": "ourheroes.inspection.utils", "qualname": "get_not_graphed", "type": "function", "doc": "<p>Generates the default set of tokens not to be graphed.</p>\n", "signature": "() -> Set[str]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.utils.get_files", "modulename": "ourheroes.inspection.utils", "qualname": "get_files", "type": "function", "doc": "<p>Gets all the files in <code>directory</code> having extension <code>ext</code>.</p>\n\n<p>Args:\n    directory: The directory from which to extract the files.\n    ext: The extension of the files to be included.</p>\n\n<p>Returns:\n    The files in <code>directory</code> having extension <code>ext</code>.</p>\n", "signature": "(directory: str, ext: str = '.txt') -> List[str]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.utils.save_json", "modulename": "ourheroes.inspection.utils", "qualname": "save_json", "type": "function", "doc": "<p>Saves a <code>dictionary</code> as a json file at <code>save_path</code>.</p>\n", "signature": "(dictionary: Dict[str, int], save_path: str)", "funcdef": "def"}, {"fullname": "ourheroes.inspection.utils.parse_dataset", "modulename": "ourheroes.inspection.utils", "qualname": "parse_dataset", "type": "function", "doc": "<p>Parses the dataset from single to multiple files.</p>\n", "signature": "(dataset_path: str) -> str", "funcdef": "def"}, {"fullname": "ourheroes.inspection.vocabs", "modulename": "ourheroes.inspection.vocabs", "type": "module", "doc": "<p>Module implementing the vocabulary retrieval routines.</p>\n"}, {"fullname": "ourheroes.inspection.vocabs.get_file_vocab", "modulename": "ourheroes.inspection.vocabs", "qualname": "get_file_vocab", "type": "function", "doc": "<p>Updates the dataset vocabulary with file tokens.</p>\n\n<p>Args:\n    vocab: The dataset vocabulary to be updated.\n    file: The file used to update the vocabulary.\n    tq: The tqdm progress bar manager.\n    encoding: The encoding of the file.\n    sections: The sections key in the file data.\n    summary: The summary key in the file data.\n    tokenize: The callable used for tokenizing.</p>\n", "signature": "(\n    vocab: collections.Counter,\n    file: str,\n    tq: tqdm.std.tqdm,\n    encoding: str = 'utf-8',\n    sections: str = 'sections',\n    summary: str = 'abstract_text',\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n)", "funcdef": "async def"}, {"fullname": "ourheroes.inspection.vocabs.load_saved_vocab", "modulename": "ourheroes.inspection.vocabs", "qualname": "load_saved_vocab", "type": "function", "doc": "<p>Loads a precomputed vocabulary.</p>\n\n<p>Args:\n    save_path: The path at which the vocabulary is saved.</p>\n\n<p>Returns:\n    The saved vocabulary.</p>\n", "signature": "(save_path: str) -> Dict[str, int]", "funcdef": "def"}, {"fullname": "ourheroes.inspection.vocabs.get_files_vocab", "modulename": "ourheroes.inspection.vocabs", "qualname": "get_files_vocab", "type": "function", "doc": "<p>Generates the dataset vocabulary.</p>\n\n<p>Args:\n    files: The files in the dataset.\n    save_path: The path to which the vocabulary is saved.\n    encoding: The encoding of the dataset files.\n    sections: The sections key in the file data.\n    summary: The summary key in the file data.\n    tokenize: The callable used for tokenizing.</p>\n\n<p>Returns:\n    The dateset vocabulary.</p>\n", "signature": "(\n    files: List[str],\n    save_path: str = 'files\\\\dataset.vocab',\n    encoding: str = 'utf-8',\n    sections: str = 'sections',\n    summary: str = 'abstract_text',\n    tokenize: Callable[[str], List[Union[int, str]]] = <function word_tokenize>\n) -> Dict[str, int]", "funcdef": "async def"}, {"fullname": "ourheroes.inspection.vocabs.get_representable", "modulename": "ourheroes.inspection.vocabs", "qualname": "get_representable", "type": "function", "doc": "<p>Returns the vocabulary of tokens representable by GloVe.</p>\n\n<p>Args:\n    glove_path: The GloVe cached vectors.\n    save_path: The path to which the vocabulary is saved.</p>\n\n<p>Returns:\n    The GloVe-representable vocabulary.</p>\n", "signature": "(\n    glove_path: str = '.vector_cache\\\\glove.840B.300d.txt',\n    save_path: str = 'files\\\\glove.vocab'\n) -> Dict[str, int]", "funcdef": "def"}, {"fullname": "ourheroes.models", "modulename": "ourheroes.models", "type": "module", "doc": "<p>Pytorch implementation of the model.</p>\n"}, {"fullname": "ourheroes.models.content_ranking", "modulename": "ourheroes.models.content_ranking", "type": "module", "doc": "<p>Module implementing the ContentRanking module of the model.</p>\n\n<p>The ContentRanker class is used to produce shorter document digests\nwhich are used to contain the size of the graphs used\nin later stages of the model, in particular during the\ngraph summarization phase. This module implements the classes\nand functions used in this phase of the summarization.</p>\n"}, {"fullname": "ourheroes.models.content_ranking.content_ranker", "modulename": "ourheroes.models.content_ranking.content_ranker", "type": "module", "doc": "<p>Module implementing the content ranking network.</p>\n\n<p>The network is used to produce shorter document digests\nwhich are used to contain the size of the graphs used\nin later stages of the model, in particular during the\ngraph summarization phase.</p>\n"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker.__init__", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker.__init__", "type": "function", "doc": "<p>Initializes the network.</p>\n\n<p>Args:\n    input_size: The size of the word embeddings.\n    hidden_size: The size of the hidden state of the\n        BiLSTM.</p>\n", "signature": "(self, input_size: int = 300, hidden_size: int = 512)", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker.section_scorer", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker.section_scorer", "type": "variable", "doc": "<p>The class performing content ranking.</p>\n\n<p>Following the description given in the reference paper,\nthis module assigns a relevance score to both sections\nand sentences. This is used to perform a first selection\nin order to produce shorter documents called digests\nwhich are then fed to the graph summarization module\nthanks to the reduced size of the associated graph. </p>\n\n<p>Attributes:\n    sentence_encoder: The sentence encoding module.\n    sentence_scorer: The sentence scoring module.\n    title_encoder: The title encoding module.\n    section_encoder: The section encoding module.\n    titled_attention: The attention module processing\n        the coupled title and section embeddings.\n    section_scorer: The section scoring module.</p>\n", "annotation": ": ourheroes.models.content_ranking.scorer.Scorer"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker.sentence_scoring", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker.sentence_scoring", "type": "function", "doc": "<p>Computes the relevance scores of the sentences.</p>\n\n<p>Args:\n    sentences: The sentences to score.</p>\n\n<p>Returns:\n    The relevance scores of each sentence.</p>\n", "signature": "(\n    self,\n    sentences: List[torch.Tensor]\n) -> Tuple[List[torch.Tensor], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker.section_scoring", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker.section_scoring", "type": "function", "doc": "<p>Computes the relevance scores of the sections.</p>\n\n<p>Args:\n    titles: The titles of the sections.\n    sections: The sections to be scored.</p>\n\n<p>Returns:\n    The relevance scores of each section.</p>\n", "signature": "(\n    self,\n    titles: List[torch.Tensor],\n    sections: List[torch.Tensor]\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.content_ranker.ContentRanker.forward", "modulename": "ourheroes.models.content_ranking.content_ranker", "qualname": "ContentRanker.forward", "type": "function", "doc": "<p>Computes the relevance scores of the sentences and sections in the documents.</p>\n\n<p>Args:\n    documents: The documents to score.</p>\n\n<p>Returns:\n    The relevance scores of the documents' sentences and sections.</p>\n", "signature": "(\n    self,\n    documents: List[List[Tuple[torch.Tensor, List[torch.Tensor]]]]\n) -> Tuple[torch.Tensor, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.scorer", "modulename": "ourheroes.models.content_ranking.scorer", "type": "module", "doc": "<p>\"Module implementing the Scorer class.</p>\n\n<p>The scorer class performs relevance scoring within the context of\nthe content ranking module.</p>\n"}, {"fullname": "ourheroes.models.content_ranking.scorer.Scorer", "modulename": "ourheroes.models.content_ranking.scorer", "qualname": "Scorer", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.content_ranking.scorer.Scorer.__init__", "modulename": "ourheroes.models.content_ranking.scorer", "qualname": "Scorer.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The expected embedding dimension.</p>\n", "signature": "(self, input_size: int)", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.scorer.Scorer.linear", "modulename": "ourheroes.models.content_ranking.scorer", "qualname": "Scorer.linear", "type": "variable", "doc": "<p>Module performing sentence or section relevance scoring.</p>\n\n<p>Attributes:\n    linear: The linear layer performing scoring.</p>\n", "annotation": ": torch.nn.modules.linear.Linear"}, {"fullname": "ourheroes.models.content_ranking.scorer.Scorer.forward", "modulename": "ourheroes.models.content_ranking.scorer", "qualname": "Scorer.forward", "type": "function", "doc": "<p>Assigns relevance scores to the sentence or section sequence.</p>\n\n<p>Args:\n    sequence: The sentence or section embeddings to be scored.\nReturns:\n    The computed relevance scores.</p>\n", "signature": "(self, sequence: List[torch.Tensor]) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.utils", "modulename": "ourheroes.models.content_ranking.utils", "type": "module", "doc": "<p>Module implementing utility functions for content ranking.</p>\n"}, {"fullname": "ourheroes.models.content_ranking.utils.extract", "modulename": "ourheroes.models.content_ranking.utils", "qualname": "extract", "type": "function", "doc": "<p>Extracts flat lists for title and sentence tensors as well as the section lengths</p>\n\n<p>Args:\n    documents: The documents for which to extract the features.</p>\n\n<p>Returns:\n    The title and the sentence tensors in flat lists, and the section lengths.</p>\n", "signature": "(\n    documents: List[List[Tuple[torch.Tensor, List[torch.Tensor]]]]\n) -> Tuple[List[torch.Tensor], List[torch.Tensor], List[int]]", "funcdef": "def"}, {"fullname": "ourheroes.models.content_ranking.utils.resection", "modulename": "ourheroes.models.content_ranking.utils", "qualname": "resection", "type": "function", "doc": "<p>Regroups sentences by section.</p>\n\n<p>Args:\n    sentences: Rearranges the sentence embeddings grouping them by section.\n    lengths: The section lengths.</p>\n\n<p>Returns:\n    The sections as variable-length tensors obtained by stacking the encoded sentences.</p>\n", "signature": "(sentences: List[torch.Tensor], lengths: List[int]) -> List[torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization", "modulename": "ourheroes.models.graph_summarization", "type": "module", "doc": "<p>Module implementing the Graph summarization network.</p>\n\n<p>The GraphSummarizer network is used to perform graph based\nextractive summarization. This module implements the\nnecessary submodules and utility functions.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "type": "module", "doc": "<p>Module implementing the graph summarization network.</p>\n\n<p>The GraphSummarizer class is used to perform graph based\nextractive summarization of documents given their digests.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.__init__", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.__init__", "type": "function", "doc": "<p>Initializes the network.</p>\n\n<p>Args:\n    n_iters: The number of iterative updates to perform.\n    recursive: Whether or not the updating modules are shared\n        between iterations (focus of our extension).\n    dropout_p: The dropout probability before the final prediction.</p>\n", "signature": "(\n    self,\n    n_iters: int = 2,\n    dropout_p: Optional[float] = 0.1,\n    recursive: bool = True\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.linear", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.linear", "type": "variable", "doc": "<p>The model performing the graph summarization routine.</p>\n\n<p>This is the network producing the final summary, trained with\nthe abstract of the papers as target. \nSince in the reference paper not better defined\nprojection matrices are said to be exploited,\nand finding ourselves at difficulty understanding how\na fixed-size matrix was employed to project variable-\nlength sequences, we opted to employ an Attention layers\nfor performing such reduction for both sentences and sections.</p>\n\n<p>Attributes:\n    sentence_encoder: The sentence encoding module.\n    section_encoder: The section encoding module.\n    sentence_projector: The sentence projecting module.\n        The same problem as for projecting sentences was encountered.\n    word_net: The word-nodes feature update module.\n    sentence_net: sentence-nodes feature update module.\n    section_net: section-nodes feature update module.\n    n_iters: The number of iterative updates to perform.\n    recursive: Whether or not the updating modules are shared\n        between iterations (focus of our extension).\n    dropout_p: The dropout probability before the final prediction.\n    linear: The layer computing the logits of each phrase being included\n        in the predicted summary.</p>\n", "annotation": ": torch.nn.modules.linear.Linear"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.prepare_sentences", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.prepare_sentences", "type": "function", "doc": "<p>Encodes the sentences.</p>\n\n<p>Args:\n    G: The graph from which the features are retrieved.</p>\n\n<p>Returns:\n    The sentences' projected features as a dictionary with tuples (section_number, sentence_number)\n    as keys and their respective fixed-size embeddings as values.</p>\n", "signature": "(\n    self,\n    G: networkx.classes.digraph.DiGraph\n) -> Dict[Tuple[int, int], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.project_sentences", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.project_sentences", "type": "function", "doc": "<p>Projects the sentences' variable-length embeddings to fixed-size ones.</p>\n\n<p>Args:\n    sentences: The sentences to be projected.\n    G: The graph from which the features are retrieved.</p>\n\n<p>Returns:\n    The sentences' projected features as a dictionary with tuples (section_number, sentence_number)\n    as keys and their respective fixed-size embeddings as values.</p>\n", "signature": "(\n    self,\n    sentences: Dict[Tuple[int, int], torch.Tensor],\n    G: networkx.classes.digraph.DiGraph\n) -> Dict[Tuple[int, int], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.project_sections", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.project_sections", "type": "function", "doc": "<p>Projects the sections' variable-length embeddings to fixed-size ones.</p>\n\n<p>Args:\n    sections: The sections to be projected.</p>\n\n<p>Returns:\n    The sections' projected features as a dictionary with the section numbers\n    as keys and their respective fixed-size embeddings as values.</p>\n", "signature": "(self, sections: Dict[int, torch.Tensor]) -> Dict[int, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.get_matrices", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.get_matrices", "type": "function", "doc": "<p>Creates the features matrices given the graph.</p>\n\n<p>Args:\n    G: The graph from which the features are retrieved.</p>\n\n<p>Returns:\n    The word, sentence and section features matrices in this order.</p>\n", "signature": "(\n    self,\n    G: networkx.classes.digraph.DiGraph\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.graph_summarizer.GraphSummarizer.forward", "modulename": "ourheroes.models.graph_summarization.graph_summarizer", "qualname": "GraphSummarizer.forward", "type": "function", "doc": "<p>Performs graph summarization with iterative features update exploiting Graph Attention Networks.</p>\n\n<p>Args:\n    G: The graph representing the document(s) to summarize.\nReturns:\n    The logits for each sentence of being included in the summary (or summaries).</p>\n", "signature": "(self, G: networkx.classes.digraph.DiGraph)", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.section_net", "modulename": "ourheroes.models.graph_summarization.section_net", "type": "module", "doc": "<p>Module implementing the section-level GAT pipeline.</p>\n\n<p>The WordNet class is a submodule implemented by the GraphSummarizer class\nin order to update section-level features.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.section_net.SectionNet", "modulename": "ourheroes.models.graph_summarization.section_net", "qualname": "SectionNet", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.graph_summarization.section_net.SectionNet.__init__", "modulename": "ourheroes.models.graph_summarization.section_net", "qualname": "SectionNet.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    sentence_size: The size of the sentence embeddings.\n    section_size: The size of the section embeddings.\n    n_heads: The number of attention heads of the GAT.\n    activation: The activation function downstream of the GAT.\n    dropout_p: The dropout probability.</p>\n", "signature": "(\n    self,\n    sentence_size: int = 640,\n    section_size: int = 512,\n    n_heads: int = 8,\n    activation: Callable[[torch.Tensor], torch.Tensor] = <function elu>,\n    dropout_p: Optional[float] = 0.1\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.section_net.SectionNet.dropout_p", "modulename": "ourheroes.models.graph_summarization.section_net", "qualname": "SectionNet.dropout_p", "type": "variable", "doc": "<p>The Pipeline for updating section-level features.</p>\n\n<p>Attributes:\n    gat_s: The sentence-to-section GAT.\n    gat_S: The section-to-section GAT.\n    activation: The activation function down stream of the GATs.\n    fusion: The features fusion layer.\n    ffn: The feed forward layer.\n    dropout_p: The dropout probability between the last Fusion and the FFN.</p>\n", "annotation": ": Optional[float]"}, {"fullname": "ourheroes.models.graph_summarization.section_net.SectionNet.forward", "modulename": "ourheroes.models.graph_summarization.section_net", "qualname": "SectionNet.forward", "type": "function", "doc": "<p>Performs the section-level features update.</p>\n\n<p>Args:\n    HS: The section embeddings.\n    Hs: The sentence embeddings.\n    s2S: The sentence-to-section edges.\n    S2S: The section-to-section edges.\nReturns:\n    The updated features.</p>\n", "signature": "(\n    self,\n    HS: torch.Tensor,\n    Hs: torch.Tensor,\n    s2S: torch.Tensor,\n    S2S: torch.Tensor\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.sentence_net", "modulename": "ourheroes.models.graph_summarization.sentence_net", "type": "module", "doc": "<p>Module implementing the sentence-level GAT pipeline.</p>\n\n<p>The WordNet class is a submodule implemented by the GraphSummarizer class\nin order to update sentence-level features.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.sentence_net.SentenceNet", "modulename": "ourheroes.models.graph_summarization.sentence_net", "qualname": "SentenceNet", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.graph_summarization.sentence_net.SentenceNet.__init__", "modulename": "ourheroes.models.graph_summarization.sentence_net", "qualname": "SentenceNet.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    word_size: The size of the word embeddings.\n    sentence_size: The size of the sentence embeddings.\n    section_size: The size of the section embeddings.\n    n_heads: The number of attention heads of the GAT.\n    activation: The activation function downstream of the GAT.\n    dropout_p: The dropout probability.</p>\n", "signature": "(\n    self,\n    word_size: int = 300,\n    sentence_size: int = 640,\n    section_size: int = 512,\n    n_heads: int = 8,\n    activation: Callable[[torch.Tensor], torch.Tensor] = <function elu>,\n    dropout_p: Optional[float] = 0.1\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.sentence_net.SentenceNet.dropout_p", "modulename": "ourheroes.models.graph_summarization.sentence_net", "qualname": "SentenceNet.dropout_p", "type": "variable", "doc": "<p>The Pipeline for updating sentence-level features.</p>\n\n<p>Attributes:\n    gat_w: The word-to-sentence Graph Attention Network.\n    gat_s: The sentence-to-sentence GAT.\n    gat_S: The section-to-sentence GAT.\n    activation: The activation function down stream of the GATs.\n    fusion1: The first features fusion layer.\n    fusion2: The second features fusion layer.\n    ffn: The feed forward layer.\n    dropout_p: The dropout probability between the last Fusion and the FFN.</p>\n", "annotation": ": Optional[float]"}, {"fullname": "ourheroes.models.graph_summarization.sentence_net.SentenceNet.forward", "modulename": "ourheroes.models.graph_summarization.sentence_net", "qualname": "SentenceNet.forward", "type": "function", "doc": "<p>Performs the sentence-level features update.</p>\n\n<p>Args:\n    Hs: The sentence embeddings.\n    Hw: The word embeddings.\n    HS: The section embeddings.\n    w2s: The word-to-sentence edges.\n    s2s: The sentence-to-sentence edges.\n    S2s: The section-to-sentence edges.\nReturns:\n    The updated features.</p>\n", "signature": "(\n    self,\n    Hs: torch.Tensor,\n    Hw: torch.Tensor,\n    HS: torch.Tensor,\n    w2s: torch.Tensor,\n    s2s: torch.Tensor,\n    S2s: torch.Tensor\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.utils", "modulename": "ourheroes.models.graph_summarization.utils", "type": "module", "doc": "<p>Module implementing utility functions used by the <code>graph_summarization</code> package.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.utils.data_to_tensor", "modulename": "ourheroes.models.graph_summarization.utils", "qualname": "data_to_tensor", "type": "function", "doc": "<p>Creates the features matrix from a dictionary of features.</p>\n\n<p>Compatibility with other methods is ensured by sorting keys at insertion.</p>\n\n<p>Args:\n    data: The dictionary to convert to matrix.</p>\n\n<p>Returns:\n    The features matrix as a torch.Tensor.</p>\n", "signature": "(data: Dict[Any, torch.Tensor]) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.utils.get_typed_edges", "modulename": "ourheroes.models.graph_summarization.utils", "qualname": "get_typed_edges", "type": "function", "doc": "<p>Creates the typed edges index for pytorch-geometric implementation of the GAT.</p>\n\n<p>Compatibility with other methods is ensured by sorting keys at insertion.</p>\n\n<p>Args:\n    G: The graph for which to create the index.\n    from_: The edges' tail-nodes' type.\n    to: The edges' head-nodes' type.\n    device: The device on which the index is created.</p>\n\n<p>Returns:\n    The included typed edges index.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    from_: str,\n    to: str,\n    device: torch.device\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.utils.prepare_words", "modulename": "ourheroes.models.graph_summarization.utils", "qualname": "prepare_words", "type": "function", "doc": "<p>Creates a dictionary of tokens and the respective embeddings.</p>\n\n<p>Args:\n    G: The graph from which to extract the word-nodes embeddings.</p>\n\n<p>Returns:\n    A dictionary with tokens as key and their respective embeddings as values.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph) -> Dict[str, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.utils.get_edges", "modulename": "ourheroes.models.graph_summarization.utils", "qualname": "get_edges", "type": "function", "doc": "<p>Returns all of the edges indexes.</p>\n\n<p>Args:\n    G: The graph for which to create the indexes.\n    device: The device on which the indexes are created.</p>\n\n<p>Returns:\n    Being w words, s sentences, and S sections, it returns\n    s2w, w2s, s2s, S2s, s2S, and S2S indexes in this order.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    device: torch.device\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.word_net", "modulename": "ourheroes.models.graph_summarization.word_net", "type": "module", "doc": "<p>Module implementing the word level GAT pipeline.</p>\n\n<p>The WordNet class is a submodule implemented by the GraphSummarizer class\nin order to update word-level features.</p>\n"}, {"fullname": "ourheroes.models.graph_summarization.word_net.WordNet", "modulename": "ourheroes.models.graph_summarization.word_net", "qualname": "WordNet", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.graph_summarization.word_net.WordNet.__init__", "modulename": "ourheroes.models.graph_summarization.word_net", "qualname": "WordNet.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    word_size: The size of the word embeddings.\n    sentence_size: The size of the sentence embeddings.\n    n_heads: The number of attention heads of the GAT.\n    activation: The activation function down-stream of the GAT.\n    dropout_p: The dropout probability.</p>\n", "signature": "(\n    self,\n    word_size: int = 300,\n    sentence_size: int = 640,\n    n_heads: int = 6,\n    activation: Callable[[torch.Tensor], torch.Tensor] = <function elu>,\n    dropout_p: Optional[float] = 0.1\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.graph_summarization.word_net.WordNet.dropout_p", "modulename": "ourheroes.models.graph_summarization.word_net", "qualname": "WordNet.dropout_p", "type": "variable", "doc": "<p>The Pipeline for updating word-level features.</p>\n\n<p>Attributes:\n    gat: The Graph Attention Network.\n    activation: The activation function down stream of the GAT.\n    ffn: The feed forward layer.\n    dropout_p: The dropout probability between GAT and FFN.</p>\n", "annotation": ": Optional[float]"}, {"fullname": "ourheroes.models.graph_summarization.word_net.WordNet.forward", "modulename": "ourheroes.models.graph_summarization.word_net", "qualname": "WordNet.forward", "type": "function", "doc": "<p>Performs the word-level features update.</p>\n\n<p>Args:\n    Hw: The word embeddings.\n    Hs: The sentence embeddings.\n    s2w: The sentence-to-word edges.\nReturns:\n    The updated features.</p>\n", "signature": "(\n    self,\n    Hw: torch.Tensor,\n    Hs: torch.Tensor,\n    s2w: torch.Tensor\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules", "modulename": "ourheroes.models.submodules", "type": "module", "doc": "<p>Module implementing utility and shared submodules.</p>\n"}, {"fullname": "ourheroes.models.submodules.attention", "modulename": "ourheroes.models.submodules.attention", "type": "module", "doc": "<p>Module implementing the Attention submodule.</p>\n\n<p>The module reproduces the attention mechanism described\nin the reference paper.</p>\n"}, {"fullname": "ourheroes.models.submodules.attention.Attention", "modulename": "ourheroes.models.submodules.attention", "qualname": "Attention", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.attention.Attention.__init__", "modulename": "ourheroes.models.submodules.attention", "qualname": "Attention.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The expected size of the input.</p>\n", "signature": "(self, input_size: int)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.attention.Attention.softmax", "modulename": "ourheroes.models.submodules.attention", "qualname": "Attention.softmax", "type": "variable", "doc": "<p>Class implementing the attention mechanism\nas describe in the reference paper.</p>\n\n<p>Attributes:\n    linear: The linear layer.\n    u_att: The learned attention query.\n    softmax: The softmax layer.</p>\n", "annotation": ": torch.nn.modules.activation.Softmax"}, {"fullname": "ourheroes.models.submodules.attention.Attention.forward", "modulename": "ourheroes.models.submodules.attention", "qualname": "Attention.forward", "type": "function", "doc": "<p>Applies the attention mechanism.</p>\n\n<p>Args:\n    x: The input.\nReturns:\n    The fixed-length combination of the input with attention weighting.</p>\n", "signature": "(self, x: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.encoder", "modulename": "ourheroes.models.submodules.encoder", "type": "module", "doc": "<p>Module implementing the base Encoder submodule.</p>\n\n<p>The Encoder submodule is used to encode variable length\nsentence and section representations into fixed size ones.</p>\n"}, {"fullname": "ourheroes.models.submodules.encoder.Encoder", "modulename": "ourheroes.models.submodules.encoder", "qualname": "Encoder", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.encoder.Encoder.__init__", "modulename": "ourheroes.models.submodules.encoder", "qualname": "Encoder.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The expected input_size.\n    hidden_size: The size of the output and of the hidden state of\n        the BiLSTM.</p>\n", "signature": "(self, input_size: int, hidden_size: int)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.encoder.Encoder.attention", "modulename": "ourheroes.models.submodules.encoder", "qualname": "Encoder.attention", "type": "variable", "doc": "<p>Encoder used by the content ranking module.</p>\n\n<p>Attributes:\n    BiLSTM: The Bidirectional LSTM module.\n    Attention: The attention module.</p>\n", "annotation": ": ourheroes.models.submodules.attention.Attention"}, {"fullname": "ourheroes.models.submodules.encoder.Encoder.forward", "modulename": "ourheroes.models.submodules.encoder", "qualname": "Encoder.forward", "type": "function", "doc": "<p>Performs encoding.</p>\n\n<p>Args:\n    tensors: The variable length tensors representing sentences or sections.\nReturns:\n    The fixed-length representation of encoded sentences or sections.</p>\n", "signature": "(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.ffn", "modulename": "ourheroes.models.submodules.ffn", "type": "module", "doc": "<p>Module implementing the FFN submodule.</p>\n\n<p>The FFN submodule is as simple two-layer feed forward network\nimplemented by the graph summarization GAT networks.</p>\n"}, {"fullname": "ourheroes.models.submodules.ffn.FFN", "modulename": "ourheroes.models.submodules.ffn", "qualname": "FFN", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.ffn.FFN.__init__", "modulename": "ourheroes.models.submodules.ffn", "qualname": "FFN.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The expected input size.\n    output_size: The size of the output.\n    hidden_size: The size of the first (resp. second) layer output (resp. input).\n    dropout_p: The probability of performing dropout between the two layers.</p>\n", "signature": "(\n    self,\n    input_size: int,\n    output_size: int,\n    hidden_size: int = 2048,\n    dropout_p: Optional[float] = 0.1\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.ffn.FFN.dropout_p", "modulename": "ourheroes.models.submodules.ffn", "qualname": "FFN.dropout_p", "type": "variable", "doc": "<p>A two layer feed forward network.</p>\n\n<p>Attributes:\n    l1: The first linear layer.\n    l2: The second linear layer.\n    dropout: The probability of dropout between the two layers.</p>\n", "annotation": ": Optional[float]"}, {"fullname": "ourheroes.models.submodules.ffn.FFN.forward", "modulename": "ourheroes.models.submodules.ffn", "qualname": "FFN.forward", "type": "function", "doc": "<p>Performs the operations described in the reference paper.</p>\n\n<p>Args:\n    U: The features processed by a GAT.\n    H: The features before processing by the GAT.\nReturns:\n    The result of feed forward and addition.</p>\n", "signature": "(self, U: torch.Tensor, H: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.fusion", "modulename": "ourheroes.models.submodules.fusion", "type": "module", "doc": "<p>Module implementing the Fusion submodule.</p>\n\n<p>The Fusion submodule is implemented by the graph summarization GAT networks\nin order to update features from multiple sources.</p>\n"}, {"fullname": "ourheroes.models.submodules.fusion.Fusion", "modulename": "ourheroes.models.submodules.fusion", "qualname": "Fusion", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.fusion.Fusion.__init__", "modulename": "ourheroes.models.submodules.fusion", "qualname": "Fusion.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The size of the expected input.</p>\n", "signature": "(self, input_size: int)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.fusion.Fusion.activation", "modulename": "ourheroes.models.submodules.fusion", "qualname": "Fusion.activation", "type": "variable", "doc": "<p>Performs features fusion as described in the reference paper.</p>\n\n<p>Attributes:\n    linear: The linear layer.\n    activation: The activation function.</p>\n", "annotation": ": collections.abc.Callable[[torch.Tensor], torch.Tensor]"}, {"fullname": "ourheroes.models.submodules.fusion.Fusion.forward", "modulename": "ourheroes.models.submodules.fusion", "qualname": "Fusion.forward", "type": "function", "doc": "<p>Performs features fusion.</p>\n\n<p>Args:\n    X: The (left) tensor of features.\n    Y: The (right) tensor of features.\nReturns:\n    The result of feature fusion.</p>\n", "signature": "(self, X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.section_encoder", "modulename": "ourheroes.models.submodules.section_encoder", "type": "module", "doc": "<p>Module implementing the SectionEncoder class.</p>\n\n<p>This submodule is implemented by the GraphSummarizer class\nin order to project variable length section representations\nto fixed-size ones.</p>\n"}, {"fullname": "ourheroes.models.submodules.section_encoder.SectionEncoder", "modulename": "ourheroes.models.submodules.section_encoder", "qualname": "SectionEncoder", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.section_encoder.SectionEncoder.__init__", "modulename": "ourheroes.models.submodules.section_encoder", "qualname": "SectionEncoder.__init__", "type": "function", "doc": "<p>Initializes the encoder.</p>\n\n<p>Args:\n    input_size: The size of the expected input.\n    hidden_size: The size of the hidden_state of the BiLSTM.\n    num_layers: The number of layers of the BiLSTM module.</p>\n", "signature": "(\n    self,\n    input_size: int = 512,\n    hidden_size: int = 256,\n    num_layers: int = 2\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.section_encoder.SectionEncoder.output_size", "modulename": "ourheroes.models.submodules.section_encoder", "qualname": "SectionEncoder.output_size", "type": "variable", "doc": "<p>Performs section encoding for purposes of graph summarization.</p>\n\n<p>Attributes:\n    attention: The Attention submodule.\n    BiLSTM: The Bidirectional LSTM module.\n    output_size: The dimension of the output embeddings.</p>\n", "annotation": ": int"}, {"fullname": "ourheroes.models.submodules.section_encoder.SectionEncoder.forward", "modulename": "ourheroes.models.submodules.section_encoder", "qualname": "SectionEncoder.forward", "type": "function", "doc": "<p>Performs section encoding.</p>\n\n<p>Args:\n    sentences: The dictionary of sentence tensors.\nReturns:\n    The dictionary of encoded sentence tensors.</p>\n", "signature": "(\n    self,\n    sentences: Dict[Tuple[int, int], torch.Tensor]\n) -> Dict[int, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.sentence_encoder", "modulename": "ourheroes.models.submodules.sentence_encoder", "type": "module", "doc": "<p>Module implementing the SentenceEncoder class.</p>\n\n<p>This submodule is implemented by the GraphSummarizer class\nin order to project variable length sentence representations\nto fixed-size ones.</p>\n"}, {"fullname": "ourheroes.models.submodules.sentence_encoder.SentenceEncoder", "modulename": "ourheroes.models.submodules.sentence_encoder", "qualname": "SentenceEncoder", "type": "class", "doc": "<p>Base class for all neural network modules.</p>\n\n<p>Your models should also subclass this class.</p>\n\n<p>Modules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::</p>\n\n<pre><code>import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n</code></pre>\n\n<p>Submodules assigned in this way will be registered, and will have their\nparameters converted too when you call <code>to</code>, etc.</p>\n\n<p>:ivar training: Boolean represents whether this module is in training or\n                evaluation mode.\n:vartype training: bool</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "ourheroes.models.submodules.sentence_encoder.SentenceEncoder.__init__", "modulename": "ourheroes.models.submodules.sentence_encoder", "qualname": "SentenceEncoder.__init__", "type": "function", "doc": "<p>Initializes the module.</p>\n\n<p>Args:\n    input_size: The size of the word embeddings.\n    kernel_sizes: The sizes of the convolutions' kernels.\n    hidden_size: The hidden size of the BiLSTM.\n    num_layers: The number of layers of the BiLSTM.</p>\n", "signature": "(\n    self,\n    input_size: int = 300,\n    kernel_output: int = 50,\n    kernel_sizes: Sequence[int] = (2, 3, 4, 5, 6, 7),\n    hidden_size: int = 256,\n    num_layers: int = 2\n)", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.sentence_encoder.SentenceEncoder.output_size", "modulename": "ourheroes.models.submodules.sentence_encoder", "qualname": "SentenceEncoder.output_size", "type": "variable", "doc": "<p>Encoder for the representation of sentences as used by the graph summarization module.</p>\n\n<p>Attributes:\n    convs: The convolution modules list.\n    inter_size: The size of the output of the convolution modules,\n        thus the input of the BiLSTM module.\n    BiLSTM: The Bidirectional LSTM.\n    output_size: The dimension of the output embeddings.</p>\n", "annotation": ": int"}, {"fullname": "ourheroes.models.submodules.sentence_encoder.SentenceEncoder.convolve", "modulename": "ourheroes.models.submodules.sentence_encoder", "qualname": "SentenceEncoder.convolve", "type": "function", "doc": "<p>Performs the convolutions on the input tensors.</p>\n\n<p>Args:\n    values: The sentences variable length representations.\nReturns:\n    The result of the convolutions.</p>\n", "signature": "(self, values: List[torch.Tensor]) -> List[torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.sentence_encoder.SentenceEncoder.forward", "modulename": "ourheroes.models.submodules.sentence_encoder", "qualname": "SentenceEncoder.forward", "type": "function", "doc": "<p>Encodes the sentences.</p>\n\n<p>Encoding is performed lest projection as described in the reference paper.</p>\n\n<p>Args:\n    sentences: The sentences to be encoded.\nReturns:\n    The encoded sentences.</p>\n", "signature": "(\n    self,\n    sentences: Dict[Tuple[int, int], torch.Tensor]\n) -> Dict[Tuple[int, int], torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.utils", "modulename": "ourheroes.models.submodules.utils", "type": "module", "doc": "<p>Module implementing utility functions used by the submodules.</p>\n"}, {"fullname": "ourheroes.models.submodules.utils.pack", "modulename": "ourheroes.models.submodules.utils", "qualname": "pack", "type": "function", "doc": "<p>Performs packing of variable length sequences.</p>\n\n<p>Args:\n    sentences: The variable length tensors to pack.\nReturns:\n    A pytorch PackedSequence along with the lengths of the tensors,\n    useful for reversing the operation.</p>\n", "signature": "(\n    sentences: List[torch.Tensor]\n) -> Tuple[torch.nn.utils.rnn.PackedSequence, List[int]]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.utils.unpack", "modulename": "ourheroes.models.submodules.utils", "qualname": "unpack", "type": "function", "doc": "<p>Performs the unpacking of a pytorch PackedSequence.</p>\n\n<p>Args:\n    packed: The PackedSequence to unpack.\n    dims: The original tensors' dimensions.\nReturns:\n    The unpacked sequence.</p>\n", "signature": "(\n    packed: torch.nn.utils.rnn.PackedSequence,\n    dims: List[int]\n) -> List[torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.utils.get_sections_lists", "modulename": "ourheroes.models.submodules.utils", "qualname": "get_sections_lists", "type": "function", "doc": "<p>Maps sentence tensors to section tensors.</p>\n\n<p>Args:\n    sentences: The dictionary of sentence tensors index at sentence level.\nReturns:\n    A dictionary index by section with List[torch.Tensor] values.</p>\n", "signature": "(\n    sentences: Dict[Tuple[int, int], torch.Tensor]\n) -> Dict[int, List[torch.Tensor]]", "funcdef": "def"}, {"fullname": "ourheroes.models.submodules.utils.get_section_tensors", "modulename": "ourheroes.models.submodules.utils", "qualname": "get_section_tensors", "type": "function", "doc": "<p>Maps sentence tensors to section tensors.</p>\n\n<p>Args:\n    sentences: The dictionary of sentence tensors index at sentence level.\nReturns:\n    A dictionary index by section with torch.Tensor values.</p>\n", "signature": "(\n    sentences: Dict[Tuple[int, int], torch.Tensor]\n) -> Dict[int, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.training", "modulename": "ourheroes.training", "type": "module", "doc": "<p>Module implementing the training and evaluation routines.</p>\n"}, {"fullname": "ourheroes.training.configuration", "modulename": "ourheroes.training.configuration", "type": "module", "doc": "<p>Module implementing the configuration class and factories.</p>\n\n<p>The Config object is used at the time of training and evaluation\nof the model to inform the training and evaluation routines.\nThe factories return the default configurations.</p>\n"}, {"fullname": "ourheroes.training.configuration.config", "modulename": "ourheroes.training.configuration.config", "type": "module", "doc": "<p>Module implementing the utility class Config.</p>\n\n<p>The Config object is used at the time of training and evaluation\nof the model to inform the training and evaluation routines.</p>\n"}, {"fullname": "ourheroes.training.configuration.config.Config", "modulename": "ourheroes.training.configuration.config", "qualname": "Config", "type": "class", "doc": "<p>Config(num_epochs: int, criterion: Callable[[Union[torch.Tensor, Any], Union[torch.Tensor, Any], Any], torch.Tensor], optimizer: Type[torch.optim.optimizer.Optimizer], save_epochs: int, save_path: str, log_path: str, clip_gradient: bool, device: torch.device, seed: Optional[int] = None, source_to_device: Optional[Callable[[Any, torch.device], NoneType]] = <function recursive_to_device at 0x0000015F4B3F18B0>, target_to_device: Optional[Callable[[Any, torch.device], NoneType]] = <function recursive_to_device at 0x0000015F4B3F18B0>, scheduler: Optional[Callable[[torch.optim.optimizer.Optimizer, int, Any], float]] = <function no_scheduler at 0x0000015F52CF00D0>, eval_path: str = 'files/last_eval_dir/', dataloader_kwargs: Optional[Dict[str, Any]] = None, criterion_kwargs: Optional[Dict[str, Any]] = None, optimizer_kwargs: Optional[Dict[str, Any]] = None, scheduler_kwargs: Optional[Dict[str, Any]] = None, gradient_clipping_kwargs: Optional[Dict[str, Any]] = None)</p>\n"}, {"fullname": "ourheroes.training.configuration.config.Config.__init__", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    num_epochs: int,\n    criterion: Callable[[Union[torch.Tensor, Any], Union[torch.Tensor, Any], Any], torch.Tensor],\n    optimizer: Type[torch.optim.optimizer.Optimizer],\n    save_epochs: int,\n    save_path: str,\n    log_path: str,\n    clip_gradient: bool,\n    device: torch.device,\n    seed: Optional[int] = None,\n    source_to_device: Optional[Callable[[Any, torch.device], NoneType]] = <function recursive_to_device>,\n    target_to_device: Optional[Callable[[Any, torch.device], NoneType]] = <function recursive_to_device>,\n    scheduler: Optional[Callable[[torch.optim.optimizer.Optimizer, int, Any], float]] = <function no_scheduler>,\n    eval_path: str = 'files/last_eval_dir/',\n    dataloader_kwargs: Optional[Dict[str, Any]] = None,\n    criterion_kwargs: Optional[Dict[str, Any]] = None,\n    optimizer_kwargs: Optional[Dict[str, Any]] = None,\n    scheduler_kwargs: Optional[Dict[str, Any]] = None,\n    gradient_clipping_kwargs: Optional[Dict[str, Any]] = None\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.configuration.config.Config.seed", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.seed", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[int]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.config.Config.source_to_device", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.source_to_device", "type": "function", "doc": "<p>Recursive function for sending (possibly nested) sequence(s) of pytorch tensor to device.</p>\n\n<p>Args:\n    data: The sequence of tensors (possibly nested) to send to device.\n    device: The device to which to send the tensors.</p>\n\n<p>Returns:\n    The sequence(s) of tensors (of the same types and hierarchy) on device.</p>\n", "signature": "(\n    data: Union[torch.Tensor, Sequence],\n    device: torch.device\n) -> Union[Sequence, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.training.configuration.config.Config.target_to_device", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.target_to_device", "type": "function", "doc": "<p>Recursive function for sending (possibly nested) sequence(s) of pytorch tensor to device.</p>\n\n<p>Args:\n    data: The sequence of tensors (possibly nested) to send to device.\n    device: The device to which to send the tensors.</p>\n\n<p>Returns:\n    The sequence(s) of tensors (of the same types and hierarchy) on device.</p>\n", "signature": "(\n    data: Union[torch.Tensor, Sequence],\n    device: torch.device\n) -> Union[Sequence, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.training.configuration.config.Config.scheduler", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.scheduler", "type": "function", "doc": "<p>Utility function for no learning rate scheduling.</p>\n\n<p>Args:\n    optimizer: The optimizer training the model.\n    _: The batch iteration.</p>\n\n<p>Returns:\n    The unchanged learning rate of the optimizer.</p>\n", "signature": "(optimizer: torch.optim.optimizer.Optimizer, _: int) -> float", "funcdef": "def"}, {"fullname": "ourheroes.training.configuration.config.Config.eval_path", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.eval_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = 'files/last_eval_dir/'"}, {"fullname": "ourheroes.training.configuration.config.Config.dataloader_kwargs", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.dataloader_kwargs", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[Dict[str, Any]]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.config.Config.criterion_kwargs", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.criterion_kwargs", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[Dict[str, Any]]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.config.Config.optimizer_kwargs", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.optimizer_kwargs", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[Dict[str, Any]]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.config.Config.scheduler_kwargs", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.scheduler_kwargs", "type": "variable", "doc": "<p></p>\n", "annotation": ": Optional[Dict[str, Any]]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.config.Config.gradient_clipping_kwargs", "modulename": "ourheroes.training.configuration.config", "qualname": "Config.gradient_clipping_kwargs", "type": "variable", "doc": "<p>Utility class for training our networks.</p>\n\n<p>Attributes:\n    num_epochs: The number of epochs for which to train.\n    criterion: The loss criterion.\n    optimizer: The optimizer to use.\n    save_epochs: The number of epochs between each checkpoint saved\n        (if epoch=save_epochs in modulo a checkpoint is created).\n    save_path: The directory in which checkpoints are saved.\n    log_path: The file used to log training information.\n    clip_gradient: Whether or not to perform gradient clipping.\n    device: The device to be used for training.\n    seed: The seed for reproducibility.\n    source_to_device: The method to transfer the source to device.\n    target_to_device: The method to transfer the target to device.\n    scheduler: The learning rate scheduler. \n    eval_path: The directory to save evaluation info.\n    dataloader_kwargs: Keyword arguments for the dataloader factory.\n    criterion_kwargs: Keyword arguments for the loss function.\n    optimizer_kwargs: Keyword arguments for the optimizer factory.\n    scheduler_kwargs: Keyword arguments for the scheduler callable.\n    gradient_clipping_kwargs: Keyword arguments for gradient clipping callable.</p>\n", "annotation": ": Optional[Dict[str, Any]]", "default_value": " = None"}, {"fullname": "ourheroes.training.configuration.factories", "modulename": "ourheroes.training.configuration.factories", "type": "module", "doc": "<p>Module implementing default configuration factories.</p>\n\n<p>The functions in this module return the default configuration\nobjects used by the training and evaluation routines. The\ndataloader kwargs are optimized for hardware used by the authors.</p>\n"}, {"fullname": "ourheroes.training.configuration.factories.default_cr_config", "modulename": "ourheroes.training.configuration.factories", "qualname": "default_cr_config", "type": "function", "doc": "<p>Creates the default configuration for training the content ranking network.</p>\n\n<p>Returns:\n    The default configuration for training the content ranking network.</p>\n", "signature": "() -> ourheroes.training.configuration.config.Config", "funcdef": "def"}, {"fullname": "ourheroes.training.configuration.factories.default_gs_config", "modulename": "ourheroes.training.configuration.factories", "qualname": "default_gs_config", "type": "function", "doc": "<p>Creates the default configuration for training the graph summarization network.</p>\n\n<p>Returns:\n    The default configuration for training the graph summarization network.</p>\n", "signature": "() -> ourheroes.training.configuration.config.Config", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting", "modulename": "ourheroes.training.digesting", "type": "module", "doc": "<p>Module implementing the digesting routine.</p>\n\n<p>The module implements the digesting routine and all the\nnecessary classes and functions.</p>\n"}, {"fullname": "ourheroes.training.digesting.app", "modulename": "ourheroes.training.digesting.app", "type": "module", "doc": "<p>Module implementing the routine to produce and save document digests.</p>\n"}, {"fullname": "ourheroes.training.digesting.app.write_digest", "modulename": "ourheroes.training.digesting.app", "qualname": "write_digest", "type": "function", "doc": "<p>Writes the document digest in the original file.</p>\n\n<p>Args:\n    fname: The location of the document file.\n    digest_: The document digest.</p>\n", "signature": "(fname: str, digest_: Dict[int, List[int]])", "funcdef": "async def"}, {"fullname": "ourheroes.training.digesting.app.digest", "modulename": "ourheroes.training.digesting.app", "qualname": "digest", "type": "function", "doc": "<p>Runs the digesting routine.</p>\n\n<p>Args:\n    model: The content ranking module.\n    digester: The digester object.\n    dataset: The dataset to digest.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    digester: ourheroes.training.digesting.digester.Digester,\n    dataset: torch.utils.data.dataset.Dataset\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.digester", "modulename": "ourheroes.training.digesting.digester", "type": "module", "doc": "<p>Module implementing the Digester class.</p>\n\n<p>The digester class is used to generate document digests\ngiven a trained ContentRanker relevance scores.</p>\n"}, {"fullname": "ourheroes.training.digesting.digester.Digester", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester", "type": "class", "doc": "<p>Digester(n_sections: int, n_sentences: int, sections: str)</p>\n"}, {"fullname": "ourheroes.training.digesting.digester.Digester.__init__", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, n_sections: int, n_sentences: int, sections: str)", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.digester.Digester.sections", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester.sections", "type": "variable", "doc": "<p>Class that creates digest numerals given the content ranking relevance scores.    </p>\n\n<p>Attributes:\n    n_sections: The maximum number of sections to be selected.\n    n_sentences: The maximum number of sentences to be selected\n        in each of the selected sections.\n    sections: The sections key in data.</p>\n", "annotation": ": str"}, {"fullname": "ourheroes.training.digesting.digester.Digester.get_indexes", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester.get_indexes", "type": "function", "doc": "<p>Retrieves from a document the sentence indexes and the number of sections.</p>\n\n<p>Args:\n    data: The document to retrieve the indexes from.</p>\n\n<p>Returns:\n    The sentence indexes and the number of sections.</p>\n", "signature": "(self, data: Dict[str, Any]) -> Tuple[List[int], int]", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.digester.Digester.digest", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester.digest", "type": "function", "doc": "<p>Creates the digest for a document given the content ranking relevance scores.</p>\n\n<p>Args:\n    sentence_scores: The relevance scores of the sentences.\n    section_scores: The relevance scores of the sections.\n    data: The Data associated with the document to be digested.\n    section_start: The index of the first document section in the flat section scores.\n    sentence_start: The index of the first document sentence in the flat section scores.</p>\n\n<p>Returns:\n    The digest of the input document, and the scores' indexes for the next document.</p>\n", "signature": "(\n    self,\n    sentence_scores: torch.Tensor,\n    section_scores: torch.Tensor,\n    data: Dict[str, Any],\n    section_start: int,\n    sentence_start: int\n) -> Tuple[Dict[int, List[int]], int, int]", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.digester.Digester.digests", "modulename": "ourheroes.training.digesting.digester", "qualname": "Digester.digests", "type": "function", "doc": "<p>Creates the digests for documents given the content ranking relevance scores.</p>\n\n<p>Args:\n    sentence_scores: The relevance scores of the sentences.\n    section_scores: The relevance scores of the sections.\n    data: The Data associated with the documents to be digested.</p>\n\n<p>Returns:\n    The digests of the input documents.</p>\n", "signature": "(\n    self,\n    sentence_scores: torch.Tensor,\n    section_scores: torch.Tensor,\n    data: List[Dict[str, Any]]\n) -> List[Dict[int, List[int]]]", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.factories", "modulename": "ourheroes.training.digesting.factories", "type": "module", "doc": "<p>Factory methods for the <code>digesting</code> package.</p>\n\n<p>The default digester retains the four most relevant\nsections for each of these only the top 30 relevant\nsentences as in the reference paper.</p>\n"}, {"fullname": "ourheroes.training.digesting.factories.default_digester", "modulename": "ourheroes.training.digesting.factories", "qualname": "default_digester", "type": "function", "doc": "<p>Creates the default Digester object.</p>\n\n<p>Returns:\n    The default Digester object.</p>\n", "signature": "() -> ourheroes.training.digesting.digester.Digester", "funcdef": "def"}, {"fullname": "ourheroes.training.digesting.utils", "modulename": "ourheroes.training.digesting.utils", "type": "module", "doc": "<p>Module implementing utility functions for the <code>digesting</code> module.</p>\n"}, {"fullname": "ourheroes.training.digesting.utils.digest_collate", "modulename": "ourheroes.training.digesting.utils", "qualname": "digest_collate", "type": "function", "doc": "<p>Collate function for the digesting app.</p>\n\n<p>Args:\n    batch: The data loaded from a DigestableDataset.</p>\n\n<p>Returns:\n    The collated batch.</p>\n", "signature": "(batch: Sequence)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval", "modulename": "ourheroes.training.eval", "type": "module", "doc": "<p>Module implementing the evaluation routines.</p>\n\n<p>The module implements the evaluation routine and all the\nnecessary classes and functions.</p>\n"}, {"fullname": "ourheroes.training.eval.eval", "modulename": "ourheroes.training.eval.eval", "type": "module", "doc": "<p>Module implementing the evaluation routine.</p>\n"}, {"fullname": "ourheroes.training.eval.eval.no_collate", "modulename": "ourheroes.training.eval.eval", "qualname": "no_collate", "type": "function", "doc": "<p></p>\n", "signature": "(batch: Sequence) -> Tuple", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.eval.evaluate", "modulename": "ourheroes.training.eval.eval", "qualname": "evaluate", "type": "function", "doc": "<p>Performs the evaluation of a model.</p>\n\n<p>Args:\n    model: The model to evaluate.\n    dataset: The dataset on which to evaluate the model.\n    evaluator: The evaluator callable assigning evaluation metrics.\n    config: The configuration used for evaluating.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    dataset: torch.utils.data.dataset.Dataset,\n    evaluator: Callable[[List[str], torch.Tensor, List[List[List[Union[int, str]]]]], Union[float, Dict[str, float]]],\n    config: ourheroes.training.configuration.config.Config\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.eval.evaluate_epoch", "modulename": "ourheroes.training.eval.eval", "qualname": "evaluate_epoch", "type": "function", "doc": "<p>Evaluates the model at a given epoch.</p>\n\n<p>Args:\n    model: The model to evaluate.\n    dataloader: The dataloader used to evaluate the model.\n    evaluator: The evaluator callable assigning evaluation metrics.\n    config: The configuration used for evaluating.\n    epoch: The currently evaluated epoch.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    dataloader: torch.utils.data.dataloader.DataLoader,\n    evaluator: Callable[[List[str], torch.Tensor, List[List[List[Union[int, str]]]]], Union[float, Dict[str, float]]],\n    config: ourheroes.training.configuration.config.Config,\n    epoch: int\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.eval.save_scores", "modulename": "ourheroes.training.eval.eval", "qualname": "save_scores", "type": "function", "doc": "<p>Saves a the current iteration evaluation scores.</p>\n\n<p>Args:\n    scores: The scores to be saved.\n    eval_dir: The directory to which the scores are saved.\n    epoch: The epoch of which the evaluation was performed.</p>\n", "signature": "(scores: Dict[str, Any], eval_dir: str, epoch: int)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator", "modulename": "ourheroes.training.eval.evaluator", "type": "module", "doc": "<p>Module implementing the evaluator classes.</p>\n\n<p>The evaluator classes are used at time of evaluation\nin order to compute the metrics given document(s) and target(s).</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.SingleEvaluator", "modulename": "ourheroes.training.eval.evaluator", "qualname": "SingleEvaluator", "type": "class", "doc": "<p>SingleEvaluator(score: ourheroes.data.scoring.scorers.MeanRougeScorer, prepare_pred: Callable[[torch.Tensor, List[List[List[Union[int, str]]]]], str], prepare_target: Callable[[List[str]], str])</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.SingleEvaluator.__init__", "modulename": "ourheroes.training.eval.evaluator", "qualname": "SingleEvaluator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    score: ourheroes.data.scoring.scorers.MeanRougeScorer,\n    prepare_pred: Callable[[torch.Tensor, List[List[List[Union[int, str]]]]], str],\n    prepare_target: Callable[[List[str]], str]\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.SingleEvaluator.prepare_target", "modulename": "ourheroes.training.eval.evaluator", "qualname": "SingleEvaluator.prepare_target", "type": "variable", "doc": "<p>Class computing a single evaluation metric.</p>\n\n<p>The class first processes the prediction and target to make\nthem apt for scoring, then computes and return the evaluation result.    </p>\n\n<p>Attributes:\n    score: The callable scorer used for computing the method.\n    prepare_pred: The prediction preparation routine.\n    prepare_target: The target preparation routine.</p>\n", "annotation": ": Callable[[List[str]], str]"}, {"fullname": "ourheroes.training.eval.evaluator.SingleEvaluator.evaluate", "modulename": "ourheroes.training.eval.evaluator", "qualname": "SingleEvaluator.evaluate", "type": "function", "doc": "<p>Evaluates prediction against target.</p>\n\n<p>Args:\n    target: The target.\n    prediction: The prediction.\n    digest: The document digest literal.</p>\n\n<p>Returns:\n    The evaluation score.</p>\n", "signature": "(\n    self,\n    target: List[str],\n    prediction: torch.Tensor,\n    digest: List[List[List[Union[int, str]]]]\n) -> float", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.MultiEvaluator", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiEvaluator", "type": "class", "doc": "<p>MultiEvaluator(evaluators: Sequence[Tuple[str, ourheroes.training.eval.evaluator.SingleEvaluator]])</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.MultiEvaluator.__init__", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiEvaluator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    evaluators: Sequence[Tuple[str, ourheroes.training.eval.evaluator.SingleEvaluator]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.MultiEvaluator.evaluators", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiEvaluator.evaluators", "type": "variable", "doc": "<p>Class computing a multiple evaluation metrics.   </p>\n\n<p>Attributes:\n    evaluators: The single evaluators to be used, a sequence of tuples\n        of evaluation type (<str>) and the corresponding SingleEvaluator.</p>\n", "annotation": ": Sequence[Tuple[str, ourheroes.training.eval.evaluator.SingleEvaluator]]"}, {"fullname": "ourheroes.training.eval.evaluator.MultiEvaluator.evaluate", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiEvaluator.evaluate", "type": "function", "doc": "<p>Evaluates prediction against target.</p>\n\n<p>Args:\n    target: The target.\n    prediction: The prediction.\n    digest: The document digest literal.</p>\n\n<p>Returns:\n    The evaluation scores as a dictionary with the evaluation types as keys.</p>\n", "signature": "(\n    self,\n    target: List[str],\n    prediction: torch.Tensor,\n    digest: List[List[List[Union[int, str]]]]\n) -> Dict[str, float]", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.MultiDocumentEvaluator", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiDocumentEvaluator", "type": "class", "doc": "<p>MultiDocumentEvaluator(evaluator: Callable[[List[str], torch.Tensor, List[List[List[Union[int, str]]]]], Union[float, Dict[str, float]]])</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.MultiDocumentEvaluator.__init__", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiDocumentEvaluator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    evaluator: Callable[[List[str], torch.Tensor, List[List[List[Union[int, str]]]]], Union[float, Dict[str, float]]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.MultiDocumentEvaluator.evaluate", "modulename": "ourheroes.training.eval.evaluator", "qualname": "MultiDocumentEvaluator.evaluate", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    targets: List[List[str]],\n    prediction: torch.Tensor,\n    digests: List[List[List[List[Union[int, str]]]]]\n) -> List[Dict]", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.Selector", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Selector", "type": "class", "doc": "<p>Selector(max_words: int)</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.Selector.__init__", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Selector.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, max_words: int)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.Selector.select", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Selector.select", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    prediction: torch.Tensor,\n    digest: List[List[List[Union[int, str]]]]\n) -> List[str]", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.flat_join", "modulename": "ourheroes.training.eval.evaluator", "qualname": "flat_join", "type": "function", "doc": "<p></p>\n", "signature": "(target: List[str]) -> str", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.newline_join", "modulename": "ourheroes.training.eval.evaluator", "qualname": "newline_join", "type": "function", "doc": "<p></p>\n", "signature": "(target: List[str]) -> str", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.Preparer", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Preparer", "type": "class", "doc": "<p>Preparer(selector: ourheroes.training.eval.evaluator.Selector, joiner: Callable[[List[str]], str])</p>\n"}, {"fullname": "ourheroes.training.eval.evaluator.Preparer.__init__", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Preparer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    selector: ourheroes.training.eval.evaluator.Selector,\n    joiner: Callable[[List[str]], str]\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.Preparer.prepare", "modulename": "ourheroes.training.eval.evaluator", "qualname": "Preparer.prepare", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    prediction: torch.Tensor,\n    digest: List[List[List[Union[int, str]]]]\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.get_base_evaluator", "modulename": "ourheroes.training.eval.evaluator", "qualname": "get_base_evaluator", "type": "function", "doc": "<p></p>\n", "signature": "(\n    rouge: str,\n    metric: str = 'fmeasure',\n    max_words: int = 200\n) -> ourheroes.training.eval.evaluator.SingleEvaluator", "funcdef": "def"}, {"fullname": "ourheroes.training.eval.evaluator.default_gs_evaluator", "modulename": "ourheroes.training.eval.evaluator", "qualname": "default_gs_evaluator", "type": "function", "doc": "<p></p>\n", "signature": "() -> ourheroes.training.eval.evaluator.MultiDocumentEvaluator", "funcdef": "def"}, {"fullname": "ourheroes.training.schedulers", "modulename": "ourheroes.training.schedulers", "type": "module", "doc": "<p>Module implementing learning rate schedulers.</p>\n"}, {"fullname": "ourheroes.training.schedulers.EpochPolyScheduler", "modulename": "ourheroes.training.schedulers", "qualname": "EpochPolyScheduler", "type": "class", "doc": "<p>EpochPolyScheduler(initial_lr: float, max_epochs: int, iterations_per_epoch: int, power: float = 0.9)</p>\n"}, {"fullname": "ourheroes.training.schedulers.EpochPolyScheduler.__init__", "modulename": "ourheroes.training.schedulers", "qualname": "EpochPolyScheduler.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    initial_lr: float,\n    max_epochs: int,\n    iterations_per_epoch: int,\n    power: float = 0.9\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.schedulers.EpochPolyScheduler.power", "modulename": "ourheroes.training.schedulers", "qualname": "EpochPolyScheduler.power", "type": "variable", "doc": "<p>Polynomial epoch-wise learning rate scheduler.</p>\n\n<p>Attributes:\n    initial_lr: The initial learning rate.\n    max_epochs: The maximum number of epochs for which the model is trained.\n    iterations_per_epoch: The number of iterations per epoch.\n    power: The exponent of the polynomial learning rate schedule.</p>\n", "annotation": ": float", "default_value": " = 0.9"}, {"fullname": "ourheroes.training.schedulers.EpochPolyScheduler.schedule", "modulename": "ourheroes.training.schedulers", "qualname": "EpochPolyScheduler.schedule", "type": "function", "doc": "<p>Computes and sets the optimizer learning rate.</p>\n\n<p>Args:\n    optimizer: The optimizer for which to set the new scheduled learning rate.\n    iteration: The iteration within the schedule.</p>\n\n<p>Returns:\n    The newly scheduled learning rate.</p>\n", "signature": "(\n    self,\n    optimizer: torch.optim.optimizer.Optimizer,\n    iteration: int\n) -> float", "funcdef": "def"}, {"fullname": "ourheroes.training.schedulers.IterationPolyScheduler", "modulename": "ourheroes.training.schedulers", "qualname": "IterationPolyScheduler", "type": "class", "doc": "<p>IterationPolyScheduler(initial_lr: float, max_iterations: int, power: float)</p>\n"}, {"fullname": "ourheroes.training.schedulers.IterationPolyScheduler.__init__", "modulename": "ourheroes.training.schedulers", "qualname": "IterationPolyScheduler.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, initial_lr: float, max_iterations: int, power: float)", "funcdef": "def"}, {"fullname": "ourheroes.training.schedulers.IterationPolyScheduler.power", "modulename": "ourheroes.training.schedulers", "qualname": "IterationPolyScheduler.power", "type": "variable", "doc": "<p>Polynomial batch-wise learning rate scheduler.</p>\n\n<p>Attributes:\n    initial_lr: The initial learning rate.\n    max_iterations: The maximum number of batch iterations for which the model is trained.\n    power: The exponent of the polynomial learning rate schedule.</p>\n", "annotation": ": float"}, {"fullname": "ourheroes.training.schedulers.IterationPolyScheduler.schedule", "modulename": "ourheroes.training.schedulers", "qualname": "IterationPolyScheduler.schedule", "type": "function", "doc": "<p>Computes and sets the optimizer learning rate.</p>\n\n<p>Args:\n    optimizer: The optimizer for which to set the new scheduled learning rate.\n    iteration: The iteration within the schedule.</p>\n\n<p>Returns:\n    The newly scheduled learning rate.</p>\n", "signature": "(\n    self,\n    optimizer: torch.optim.optimizer.Optimizer,\n    iteration: int\n) -> float", "funcdef": "def"}, {"fullname": "ourheroes.training.schedulers.no_scheduler", "modulename": "ourheroes.training.schedulers", "qualname": "no_scheduler", "type": "function", "doc": "<p>Utility function for no learning rate scheduling.</p>\n\n<p>Args:\n    optimizer: The optimizer training the model.\n    _: The batch iteration.</p>\n\n<p>Returns:\n    The unchanged learning rate of the optimizer.</p>\n", "signature": "(optimizer: torch.optim.optimizer.Optimizer, _: int) -> float", "funcdef": "def"}, {"fullname": "ourheroes.training.train", "modulename": "ourheroes.training.train", "type": "module", "doc": "<p>Module implementing the general training routine.</p>\n\n<p>The fit function can be used to train both the ContentRanker and\nthe GraphSummarizer networks.</p>\n"}, {"fullname": "ourheroes.training.train.fit", "modulename": "ourheroes.training.train", "qualname": "fit", "type": "function", "doc": "<p>Fits a model to a given dataset.</p>\n\n<p>Args:\n    model: The model to be fitted.\n    dataset: The dataset to fit the model to.\n    config: The configuration for training the model.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    dataset: torch.utils.data.dataset.Dataset,\n    config: ourheroes.training.configuration.config.Config\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.train.iterate", "modulename": "ourheroes.training.train", "qualname": "iterate", "type": "function", "doc": "<p>Carries out an epoch for training a model with a given dataset.</p>\n\n<p>Args:\n    model: The model to be fitted.\n    dataloader: The dataloader from which the batches are retrieved.\n    optimizer: The optimizer used for training the model.\n    scaler: The scaler used while training.\n    epoch: The epoch to perform.\n    tq: The progress bar manager.\n    config: The configuration with which to train the model.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    dataloader: torch.utils.data.dataloader.DataLoader,\n    optimizer: torch.optim.optimizer.Optimizer,\n    scaler: torch.cuda.amp.grad_scaler.GradScaler,\n    epoch: int,\n    tq: tqdm.std.tqdm,\n    config: ourheroes.training.configuration.config.Config\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils", "modulename": "ourheroes.training.utils", "type": "module", "doc": "<p>Module implementing the utility functions used for training.</p>\n"}, {"fullname": "ourheroes.training.utils.tensor_to_device", "modulename": "ourheroes.training.utils", "qualname": "tensor_to_device", "type": "function", "doc": "<p>Wrapper function for pytorch tensor to device.</p>\n\n<p>Args:\n    tensor: The tensor to send to device.\n    device: The device to which to send the tensor.</p>\n\n<p>Returns:\n    The tensor on device.</p>\n", "signature": "(tensor: torch.Tensor, device: torch.device) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.recursive_to_device", "modulename": "ourheroes.training.utils", "qualname": "recursive_to_device", "type": "function", "doc": "<p>Recursive function for sending (possibly nested) sequence(s) of pytorch tensor to device.</p>\n\n<p>Args:\n    data: The sequence of tensors (possibly nested) to send to device.\n    device: The device to which to send the tensors.</p>\n\n<p>Returns:\n    The sequence(s) of tensors (of the same types and hierarchy) on device.</p>\n", "signature": "(\n    data: Union[torch.Tensor, Sequence],\n    device: torch.device\n) -> Union[Sequence, torch.Tensor]", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.log", "modulename": "ourheroes.training.utils", "qualname": "log", "type": "function", "doc": "<p>Logs info to file.</p>\n\n<p>Args:\n    info: The info to log.\n    log_path: The file path to log to.</p>\n", "signature": "(info: Any, log_path: str)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.save_checkpoint", "modulename": "ourheroes.training.utils", "qualname": "save_checkpoint", "type": "function", "doc": "<p>Saves a model and optimizer states checkpoint.</p>\n\n<p>Args:\n    model: The model to save.\n    optimizer: The optimizer to save.\n    epoch: The current epoch.\n    directory: The directory in which to save</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    optimizer: torch.optim.optimizer.Optimizer,\n    epoch: int,\n    directory: str\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.get_checkpoints", "modulename": "ourheroes.training.utils", "qualname": "get_checkpoints", "type": "function", "doc": "<p>Retrieves the checkpoints in directory (if any).</p>\n\n<p>Args:\n    directory: The directory from which to retrieve the checkpoints.</p>\n\n<p>Returns:\n    All the checkpoints in the directory.</p>\n", "signature": "(directory: str) -> Any", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.load_checkpoint", "modulename": "ourheroes.training.utils", "qualname": "load_checkpoint", "type": "function", "doc": "<p>Loads a model and optimizer checkpoint.</p>\n\n<p>Args:\n    checkpoint_path: The path of the checkpoint to be loaded.\n    model: The model to initialize with the saved checkpoint.\n    optimizer: The model to initialize with the saved checkpoint.</p>\n\n<p>Returns:\n    The last epoch.</p>\n", "signature": "(\n    checkpoint_path: Union[str, bytes, os.PathLike],\n    model: torch.nn.modules.module.Module,\n    optimizer: Optional[torch.optim.optimizer.Optimizer] = None\n) -> int", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.load_last", "modulename": "ourheroes.training.utils", "qualname": "load_last", "type": "function", "doc": "<p>Loads the most recent epoch checkpoint (if any).</p>\n\n<p>Args:\n    directory: The directory in which the checkpoints are saved.\n    model: The model to initialize with the last checkpoint.\n    optimizer: The optimizer to initialize with the last checkpoint.</p>\n\n<p>Returns:\n    The next epoch to be performed.</p>\n", "signature": "(\n    directory: str,\n    model: torch.nn.modules.module.Module,\n    optimizer: Optional[torch.optim.optimizer.Optimizer] = None\n) -> int", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.zero_grad", "modulename": "ourheroes.training.utils", "qualname": "zero_grad", "type": "function", "doc": "<p>Zeroes the gradient in the model.</p>\n\n<p>This method is equivalent to optimizer.zero_grad() but suggested by\npytorch for speeding up the training process.</p>\n\n<p>Args:\n    model: The model of which the gradient is to be zeroed.</p>\n", "signature": "(model: torch.nn.modules.module.Module)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.clip_gradient", "modulename": "ourheroes.training.utils", "qualname": "clip_gradient", "type": "function", "doc": "<p>Performs gradient clipping.</p>\n\n<p>Args:\n    model: The model for which gradient clipping is performed.\n    optimizer: The optimizer used to train the model.\n    scaler: The scaler used while training the model.\n    **kwargs: The keyword arguments to be used for pytorch's clip_grad_norm_ function.</p>\n", "signature": "(\n    model: torch.nn.modules.module.Module,\n    optimizer: torch.optim.optimizer.Optimizer,\n    scaler: torch.cuda.amp.grad_scaler.GradScaler,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.seed_torch", "modulename": "ourheroes.training.utils", "qualname": "seed_torch", "type": "function", "doc": "<p>Seeds pytorch rngs.</p>\n\n<p>Args:\n    seed: The seed to be used.</p>\n", "signature": "(seed: int)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.cr_loss", "modulename": "ourheroes.training.utils", "qualname": "cr_loss", "type": "function", "doc": "<p>Computes the BCE with logits loss for training the content ranking module.</p>\n\n<p>Args:\n    prediction: The predicted sentence and section relevance scores.\n    target: The target sentence and section relevance scores.\n    **kwargs: The keyword arguments used in pytorch's binary_cross_entropy_with_logits function.</p>\n\n<p>Returns:\n    The total section and sentence loss.</p>\n", "signature": "(\n    prediction: Tuple[torch.Tensor, torch.Tensor],\n    target: Tuple[torch.Tensor, torch.Tensor],\n    **kwargs\n) -> torch.Tensor", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.cr_collate", "modulename": "ourheroes.training.utils", "qualname": "cr_collate", "type": "function", "doc": "<p>Collates a batch for training the content ranking module.</p>\n\n<p>Args:\n    batch: The batch to collate.</p>\n\n<p>Returns:\n    The collated batch.</p>\n", "signature": "(batch: Sequence)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.relabel_node", "modulename": "ourheroes.training.utils", "qualname": "relabel_node", "type": "function", "doc": "<p>Creates the new node identifier for creating a batched graph.</p>\n\n<p>Args:\n    node: The node to relabel.\n    prefix: The prefix for the node.</p>\n\n<p>Returns:\n    A tuple with as first element the prefix and subsequent element(s)\n    the original node identifier(s).</p>\n", "signature": "(\n    node: Union[int, Tuple[int, int], str],\n    prefix: int\n) -> Union[Tuple[int, int], Tuple[int, int, int], Tuple[int, str]]", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.relabel_nodes", "modulename": "ourheroes.training.utils", "qualname": "relabel_nodes", "type": "function", "doc": "<p>Relabels all nodes for creating a batched graph.</p>\n\n<p>Args:\n    nodes: The nodes to relabel.\n    prefix: The prefix for the nodes.</p>\n\n<p>Returns:\n    The list of relabelled nodes and their data.</p>\n", "signature": "(nodes: Sequence[Tuple], prefix: int) -> List", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.relabel_edges", "modulename": "ourheroes.training.utils", "qualname": "relabel_edges", "type": "function", "doc": "<p>Relabels all edges for creating a batched graph.</p>\n\n<p>Args:\n    edges: The edges to relabel.\n    prefix: The prefix for the edges' nodes.</p>\n\n<p>Returns:\n    The relabelled edges and their data.</p>\n", "signature": "(edges: Dict, prefix: int) -> List", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.join_graphs", "modulename": "ourheroes.training.utils", "qualname": "join_graphs", "type": "function", "doc": "<p>Creates a single graph representing the batch.</p>\n\n<p>Args:\n    graphs: The graphs to be included.</p>\n\n<p>Returns:\n    A single graph representing a whole batch with as isolated components\n    as there are input graphs.</p>\n", "signature": "(\n    graphs: List[networkx.classes.digraph.DiGraph]\n) -> networkx.classes.digraph.DiGraph", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.gs_collate", "modulename": "ourheroes.training.utils", "qualname": "gs_collate", "type": "function", "doc": "<p>Collates a batch for the graph summarization module.</p>\n\n<p>Args:\n    batch: The batch to collate.</p>\n\n<p>Returns:\n    The collated batch.</p>\n", "signature": "(batch: Sequence)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.attribute_to_device", "modulename": "ourheroes.training.utils", "qualname": "attribute_to_device", "type": "function", "doc": "<p>Sends nodes data to device.</p>\n\n<p>Args:\n    G: The graph for which the data is to be sent to device.\n    attribute: The attribute to send to device.\n    device: The device to which the data is sent.</p>\n", "signature": "(\n    G: networkx.classes.digraph.DiGraph,\n    attribute: str,\n    device: torch.device\n)", "funcdef": "def"}, {"fullname": "ourheroes.training.utils.graph_to_device", "modulename": "ourheroes.training.utils", "qualname": "graph_to_device", "type": "function", "doc": "<p>Sends the data of the graph to device.</p>\n\n<p>Args:\n    G: The graph for which to send data to device.\n    device: The device to which the data is sent.</p>\n\n<p>Returns:\n    The graph with data located on device.</p>\n", "signature": "(G: networkx.classes.digraph.DiGraph, device: torch.device)", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();